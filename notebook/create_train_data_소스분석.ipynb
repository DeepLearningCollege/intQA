{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import preprocessing.constants as constants\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "from preprocessing.dataset_files_saver import *\n",
    "from preprocessing.dataset_files_wrapper import *\n",
    "from preprocessing.file_util import *\n",
    "from preprocessing.raw_training_data import *\n",
    "from preprocessing.spacy_util import create_tokenizer\n",
    "from preprocessing.string_category import *\n",
    "from preprocessing.vocab import get_vocab\n",
    "from util.string_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BOS = \"bos\"\n",
    "_EOS = \"eos\"\n",
    "\n",
    "_DEBUG_USE_ONLY_FIRST_ARTICLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Some of the training/dev data seems to be inaccurate. This code\n",
    "# tries to make sure that at least one of the \"qa\" options in the acceptable\n",
    "# answers list is accurate and includes it in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPosition:\n",
    "    def __init__(self, start_idx, end_idx):\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageContext:\n",
    "    '''Class used to save the tokenization positions in a given passage\n",
    "       so that the original strings can be used for constructing answer\n",
    "       spans rather than joining tokenized strings, which isn't 100% correct.\n",
    "    '''\n",
    "    def __init__(self, passage_str, word_id_to_text_positions,\n",
    "        acceptable_gnd_truths):\n",
    "        self.passage_str = passage_str\n",
    "        self.word_id_to_text_positions = word_id_to_text_positions\n",
    "        self.acceptable_gnd_truths = acceptable_gnd_truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data, Dev Data 생성의 결과물을 저장할 파일들 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "download_dir = \"../downloads\"\n",
    "value_idx = 0\n",
    "question_id = 0\n",
    "ner_categories = StringCategory()\n",
    "pos_categories = StringCategory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.TRAIN_FOLDER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.DEV_FOLDER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = os.path.join(data_dir, constants.TRAIN_FOLDER_NAME)\n",
    "dev_folder = os.path.join(data_dir, constants.DEV_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question.%d.npy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context.%d.npy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.CONTEXT_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'span.%d.npy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.SPAN_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_in_question.%d.npy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.WORD_IN_QUESTION_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_in_context.%d.npy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.WORD_IN_CONTEXT_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question_ids.%d.npy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question_ids_to_gnd_truths.%d'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_TO_GND_TRUTHS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context.pos.%d.npy'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.CONTEXT_POS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question.pos.%d.npy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_POS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context.ner.%d.npy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.CONTEXT_NER_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question.ner.%d.npy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_NER_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question_ids_to_squad_question_id.%d'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_TO_SQUAD_QUESTION_ID_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'passage_context.%d'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_TO_PASSAGE_CONTEXT_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & dev data already exist.\n"
     ]
    }
   ],
   "source": [
    "train_files_wrapper = DatasetFilesWrapper(train_folder)\n",
    "dev_files_wrapper = DatasetFilesWrapper(dev_folder)\n",
    "\n",
    "if all([len(os.listdir(f)) > 0 for f in [train_folder, dev_folder]]):\n",
    "    print(\"Train & dev data already exist.\")\n",
    "    #return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocabulary\n",
      "Vocab size: 2196016\n",
      "Finished getting vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting vocabulary\")\n",
    "vocab = get_vocab(data_dir)\n",
    "print(\"Finished getting vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy 는 NLTK 와 같은 토크나이저 라이브러리... 참고 http://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy 가 가진 장점이 있지만...커버링하지 못하는 부분이 여전히 존재하여...커스터마이징이 필요한 부분이 있음.\n",
    "spacy_util.py 주석 참고...\n",
    "\n",
    "    # The following way of definining unicode characters that should be\n",
    "    # tokenized is super ugly and I would hope that it can be improved.\n",
    "    # But it is better than not doing it because spacy's tokenizer won't break\n",
    "    # on these  weird characters when it should.\n",
    "    # To get this list, I scraped the train dataset for all unicode-looking\n",
    "    # things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(nlp)\n",
    "nlp.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev-v1.1.json'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.DEV_SQUAD_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Returns (contexts, word_in_question, questions, word_in_context, spans)\\n    contexts: list of lists of integer word ids\\n    word_in_question: list of lists of booleans indicating whether each\\n        word in the context is present in the question\\n    questions: list of lists of integer word ids\\n    word_in_context: list of lists of booleans indicating whether each\\n        word in the question is present in the context\\n    spans: numpy array of shape (num_samples, 2)\\n    question_ids: a list of ints that indicates which question the\\n        given sample is part of. this has the same length as\\n        |contexts| and |questions|. multiple samples may come from\\n        the same question because there are potentially multiple valid\\n        answers for the same question\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Returns (contexts, word_in_question, questions, word_in_context, spans)\n",
    "    contexts: list of lists of integer word ids\n",
    "    word_in_question: list of lists of booleans indicating whether each\n",
    "        word in the context is present in the question\n",
    "    questions: list of lists of integer word ids\n",
    "    word_in_context: list of lists of booleans indicating whether each\n",
    "        word in the question is present in the context\n",
    "    spans: numpy array of shape (num_samples, 2)\n",
    "    question_ids: a list of ints that indicates which question the\n",
    "        given sample is part of. this has the same length as\n",
    "        |contexts| and |questions|. multiple samples may come from\n",
    "        the same question because there are potentially multiple valid\n",
    "        answers for the same question\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_data_from_tokens_list(tokens_list, tokens_ner_dict):\n",
    "    \"\"\"Input: A spaCy doc.\n",
    "\n",
    "       Ouptut: (vocab_ids_list, vocab_ids_set, pos_list, ner_list)\n",
    "    \"\"\"\n",
    "    vocab_ids_list = []\n",
    "    vocab_ids_set = set()\n",
    "    pos_list = []\n",
    "    ner_list = []\n",
    "    for zz in range(len(tokens_list)):\n",
    "        token = tokens_list[zz]\n",
    "        vocab_id = None\n",
    "        token_pos = None\n",
    "        token_ner = None\n",
    "        if not isinstance(token, spacy.tokens.token.Token) and token == _BOS:\n",
    "            vocab_id = vocab.BOS_ID\n",
    "            token_pos = \"bos\"\n",
    "            token_ner = \"bos\"\n",
    "        elif not isinstance(token, spacy.tokens.token.Token) and token == _EOS:\n",
    "            vocab_id = vocab.EOS_ID\n",
    "            token_pos = \"eos\"\n",
    "            token_ner = \"eos\"\n",
    "        else:\n",
    "            word = token.text\n",
    "            vocab_id = vocab.get_id_for_word(word)\n",
    "            token_pos = token.pos_\n",
    "            token_ner = tokens_ner_dict[token.idx].label_ \\\n",
    "                if token.idx in tokens_ner_dict else \"none\"\n",
    "            vocab_ids_set.add(vocab_id)\n",
    "        vocab_ids_list.append(vocab_id)\n",
    "        pos_list.append(pos_categories.get_id_for_word(token_pos))\n",
    "        ner_list.append(ner_categories.get_id_for_word(token_ner))\n",
    "    return vocab_ids_list, vocab_ids_set, pos_list, ner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_add_samples(value_idx, tok_context=None, tok_question=None, qa=None,\n",
    "    ctx_offset_dict=None, ctx_end_offset_dict=None, list_contexts=None,\n",
    "    list_word_in_question=None, list_questions=None,\n",
    "    list_word_in_context=None, spans=None, num_values=None,\n",
    "    question_ids=None,\n",
    "    context_pos=None,\n",
    "    question_pos=None, context_ner=None, question_ner=None,\n",
    "    is_dev=None, ctx_ner_dict=None, qst_ner_dict=None,\n",
    "    psg_ctx=None):\n",
    "    first_answer = True\n",
    "    for answer in qa[\"answers\"]:\n",
    "        answer_start = answer[\"answer_start\"]\n",
    "        text = answer[\"text\"]\n",
    "        answer_end = answer_start + len(text)\n",
    "        tok_start = None\n",
    "        tok_end = None\n",
    "        exact_match = answer_start in ctx_offset_dict and answer_end in ctx_end_offset_dict\n",
    "        if not exact_match:\n",
    "            # Sometimes, the given answer isn't actually in the context.\n",
    "            # If so, find the smallest surrounding text instead.\n",
    "            for z in range(len(tok_context)):\n",
    "                tok = tok_context[z]\n",
    "                if not isinstance(tok, spacy.tokens.token.Token):\n",
    "                    continue\n",
    "                st = tok.idx\n",
    "                end = st + len(tok.text)\n",
    "                if st <= answer_start and answer_start <= end:\n",
    "                    tok_start = tok\n",
    "                    if z == len(tok_context) - 2:\n",
    "                        tok_end = tok\n",
    "                elif tok_start is not None:\n",
    "                    tok_end = tok\n",
    "                    if end >= answer_end:\n",
    "                        break\n",
    "        tok_start = tok_start if tok_start is not None else ctx_offset_dict[answer_start]\n",
    "        tok_end = tok_end if tok_end is not None else ctx_end_offset_dict[answer_end]\n",
    "        tok_start_idx, tok_end_idx = None, None\n",
    "        for z in range(len(tok_context)):\n",
    "            tok = tok_context[z]\n",
    "            if not isinstance(tok, spacy.tokens.token.Token): # BOS, EOS\n",
    "                continue\n",
    "            if tok == tok_start:\n",
    "                tok_start_idx = z\n",
    "            if tok == tok_end:\n",
    "                tok_end_idx = z\n",
    "            if tok_start_idx is not None and tok_end_idx is not None:\n",
    "                break\n",
    "        assert(tok_start_idx is not None)\n",
    "        assert(tok_end_idx is not None)\n",
    "        # For dev, only keep one exmaple per question, and the set of all\n",
    "        # acceptable answers. This reduces the required memory for storing\n",
    "        # data.\n",
    "        if is_dev and not first_answer:\n",
    "            continue\n",
    "        first_answer = False\n",
    "\n",
    "        spans.append([tok_start_idx, tok_end_idx])\n",
    "        question_ids.append(question_id)\n",
    "\n",
    "        ctx_vocab_ids_list, ctx_vocab_ids_set, \\\n",
    "            ctx_pos_list, ctx_ner_list = \\\n",
    "            _parse_data_from_tokens_list(tok_context, ctx_ner_dict)\n",
    "        list_contexts.append(ctx_vocab_ids_list)\n",
    "        context_pos.append(ctx_pos_list)\n",
    "        context_ner.append(ctx_ner_list)\n",
    "\n",
    "        qst_vocab_ids_list, qst_vocab_ids_set, \\\n",
    "            qst_pos_list, qst_ner_list = \\\n",
    "            _parse_data_from_tokens_list(tok_question, qst_ner_dict)\n",
    "        list_questions.append(qst_vocab_ids_list)\n",
    "        question_pos.append(qst_pos_list)\n",
    "        question_ner.append(qst_ner_list)\n",
    "\n",
    "        word_in_question_list = [1 if word_id in qst_vocab_ids_set else 0 for word_id in ctx_vocab_ids_list]\n",
    "        word_in_context_list = [1 if word_id in ctx_vocab_ids_set else 0 for word_id in qst_vocab_ids_list]\n",
    "        list_word_in_question.append(word_in_question_list)\n",
    "        list_word_in_context.append(word_in_context_list)\n",
    "        print(\"Value\", value_idx, \"of\", num_values, \"percent done\",\n",
    "              100 * float(value_idx) / float(num_values), end=\"\\r\")\n",
    "        value_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_num_data_values(dataset):\n",
    "    numb_values = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                numb_values += 1\n",
    "    return numb_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ner_dict(doc):\n",
    "    d = {}\n",
    "    for e in doc.ents:\n",
    "        d[e.start_char] = e\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = constants.DEV_SQUAD_FILE\n",
    "is_dev = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from file ../downloads/dev-v1.1.json\n",
      "length of data :  48\n",
      "Squad DEV DataSet file() ../downloads/dev-v1.1.json)  num_values : 10570\n",
      "Value 0 of 10570 percent done 0.0 of 10570 percent done 0.0 of 10570 percent done 0.0of 10570 percent done 0.00.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n    return RawTrainingData(\\n        list_contexts = list_contexts,\\n        list_word_in_question = list_word_in_question,\\n        list_questions = list_questions,\\n        list_word_in_context = list_word_in_context,\\n        spans = spans,\\n        question_ids = question_ids,\\n        context_pos = context_pos,\\n        question_pos = question_pos,\\n        context_ner = context_ner,\\n        question_ner = question_ner,\\n        question_ids_to_squad_question_id = question_ids_to_squad_question_id,\\n        question_ids_to_passage_context = question_ids_to_passage_context)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.join(download_dir, data_file)\n",
    "print(\"Reading data from file\", filename)\n",
    "with open(filename) as data_file: \n",
    "    data = json.load(data_file)\n",
    "    dataset = data[\"data\"]\n",
    "    print(\"length of data : \" , str(len(dataset)))\n",
    "    num_values = _get_num_data_values(dataset)\n",
    "    print(\"Squad DEV DataSet file()\" , filename + \") \" , \"num_values :\" , num_values)\n",
    "    spans = []\n",
    "    list_contexts = []\n",
    "    list_word_in_question = []\n",
    "    list_questions = []\n",
    "    list_word_in_context = []\n",
    "    question_ids = []\n",
    "    context_pos = []\n",
    "    question_pos = []\n",
    "    context_ner = []\n",
    "    question_ner = []\n",
    "    question_ids_to_squad_question_id = {}\n",
    "    question_ids_to_passage_context = {}\n",
    "    value_idx = 0\n",
    "    for dataset_id in range(len(dataset)):\n",
    "        if dataset_id > 0 and _DEBUG_USE_ONLY_FIRST_ARTICLE:\n",
    "            break\n",
    "        article = dataset[dataset_id]\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            tok_context = nlp(context)\n",
    "            tok_contexts_with_bos_and_eos = []\n",
    "            ctx_ner_dict = _get_ner_dict(tok_context)\n",
    "            assert tok_context is not None\n",
    "            ctx_offset_dict = {}\n",
    "            ctx_end_offset_dict = {}\n",
    "            word_idx_to_text_position = {}\n",
    "\n",
    "            word_idx = 0\n",
    "            for sentence in tok_context.sents:\n",
    "                tok_contexts_with_bos_and_eos.append(_BOS)\n",
    "                word_idx_to_text_position[word_idx] = \\\n",
    "                    TextPosition(0, 0)\n",
    "                word_idx += 1\n",
    "                for token in sentence:\n",
    "                    tok_contexts_with_bos_and_eos.append(token)\n",
    "                    st = token.idx\n",
    "                    end = token.idx + len(token.text)\n",
    "                    ctx_offset_dict[st] = token\n",
    "                    ctx_end_offset_dict[end] = token\n",
    "                    word_idx_to_text_position[word_idx] = \\\n",
    "                        TextPosition(st, end)\n",
    "                    word_idx += 1\n",
    "                tok_contexts_with_bos_and_eos.append(_EOS)\n",
    "                word_idx_to_text_position[word_idx] = \\\n",
    "                    TextPosition(0, 0)\n",
    "                word_idx += 1\n",
    "\n",
    "#                    word_idx = 0\n",
    "#                    tok_contexts_with_bos_and_eos.append(_BOS)\n",
    "#                    word_idx_to_text_position[word_idx] = \\\n",
    "#                        TextPosition(0, 0)\n",
    "#                    word_idx += 1\n",
    "#                    for token in tok_context:\n",
    "#                        tok_contexts_with_bos_and_eos.append(token)\n",
    "#                        st = token.idx\n",
    "#                        end = token.idx + len(token.text)\n",
    "#                        ctx_offset_dict[st] = token\n",
    "#                        ctx_end_offset_dict[end] = token\n",
    "#                        word_idx_to_text_position[word_idx] = \\\n",
    "#                            TextPosition(st, end)\n",
    "#                        word_idx += 1\n",
    "#                    tok_contexts_with_bos_and_eos.append(_EOS)\n",
    "#                    word_idx_to_text_position[word_idx] = \\\n",
    "#                        TextPosition(0, 0)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_id += 1\n",
    "                acceptable_gnd_truths = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    acceptable_gnd_truths.append(answer[\"text\"])\n",
    "                question_ids_to_passage_context[question_id] = \\\n",
    "                    PassageContext(context, word_idx_to_text_position,\n",
    "                        acceptable_gnd_truths)\n",
    "                question = qa[\"question\"]\n",
    "                squad_question_id = qa[\"id\"]\n",
    "                assert squad_question_id is not None\n",
    "                question_ids_to_squad_question_id[question_id] = \\\n",
    "                    squad_question_id\n",
    "                tok_question = nlp(question)\n",
    "                tok_question_with_bos_and_eos = []\n",
    "\n",
    "                for sentence in tok_question.sents:\n",
    "                    tok_question_with_bos_and_eos.append(_BOS)\n",
    "                    for token in sentence:\n",
    "                        tok_question_with_bos_and_eos.append(token)\n",
    "                    tok_question_with_bos_and_eos.append(_EOS)\n",
    "\n",
    "#                        tok_question_with_bos_and_eos.append(_BOS)\n",
    "#                        for token in tok_question:\n",
    "#                            tok_question_with_bos_and_eos.append(token)\n",
    "#                        tok_question_with_bos_and_eos.append(_EOS)\n",
    "\n",
    "                qst_ner_dict = _get_ner_dict(tok_question)\n",
    "                assert tok_question is not None\n",
    "                found_answer_in_context = False\n",
    "                found_answer_in_context = _maybe_add_samples(\n",
    "                    value_idx,\n",
    "                    tok_context=tok_contexts_with_bos_and_eos,\n",
    "                    tok_question=tok_question_with_bos_and_eos, qa=qa,\n",
    "                    ctx_offset_dict=ctx_offset_dict,\n",
    "                    ctx_end_offset_dict=ctx_end_offset_dict,\n",
    "                    list_contexts=list_contexts,\n",
    "                    list_word_in_question=list_word_in_question,\n",
    "                    list_questions=list_questions,\n",
    "                    list_word_in_context=list_word_in_context,\n",
    "                    spans=spans, num_values=num_values,\n",
    "                    question_ids=question_ids,\n",
    "                    context_pos=context_pos, question_pos=question_pos,\n",
    "                    context_ner=context_ner, question_ner=question_ner,\n",
    "                    is_dev=is_dev,\n",
    "                    ctx_ner_dict=ctx_ner_dict,\n",
    "                    qst_ner_dict=qst_ner_dict,\n",
    "                    psg_ctx=question_ids_to_passage_context[question_id])\n",
    "    print(\"\")\n",
    "    spans = np.array(spans[:value_idx], dtype=np.int32)\n",
    "\"\"\" \n",
    "    return RawTrainingData(\n",
    "        list_contexts = list_contexts,\n",
    "        list_word_in_question = list_word_in_question,\n",
    "        list_questions = list_questions,\n",
    "        list_word_in_context = list_word_in_context,\n",
    "        spans = spans,\n",
    "        question_ids = question_ids,\n",
    "        context_pos = context_pos,\n",
    "        question_pos = question_pos,\n",
    "        context_ner = context_ner,\n",
    "        question_ner = question_ner,\n",
    "        question_ids_to_squad_question_id = question_ids_to_squad_question_id,\n",
    "        question_ids_to_passage_context = question_ids_to_passage_context)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DEV dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-837d58dce194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Getting DEV dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dev_raw_data = self._create_train_data_internal(\n\u001b[0m\u001b[1;32m      3\u001b[0m     constants.DEV_SQUAD_FILE, is_dev=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Getting DEV dataset\")\n",
    "dev_raw_data = self._create_train_data_internal(\n",
    "    constants.DEV_SQUAD_FILE, is_dev=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting TRAIN dataset\")\n",
    "train_raw_data = self._create_train_data_internal(\n",
    "    constants.TRAIN_SQUAD_FILE, is_dev=False)\n",
    "print(\"Num NER categories\", self.ner_categories.get_num_categories())\n",
    "print(\"Num POS categories\", self.pos_categories.get_num_categories())\n",
    "\n",
    "max_context_length = max(\n",
    "        max([len(x) for x in train_raw_data.list_contexts]),\n",
    "        max([len(x) for x in dev_raw_data.list_contexts]))\n",
    "\n",
    "max_question_length = max(\n",
    "        max([len(x) for x in train_raw_data.list_questions]),\n",
    "        max([len(x) for x in dev_raw_data.list_questions]))\n",
    "\n",
    "print(\"Saving TRAIN data\")\n",
    "train_file_saver = DatasetFilesSaver(\n",
    "        train_files_wrapper,\n",
    "        max_context_length,\n",
    "        max_question_length,\n",
    "        self.vocab,\n",
    "        train_raw_data)\n",
    "train_file_saver.save()\n",
    "\n",
    "print(\"Saving DEV data\")\n",
    "dev_file_saver = DatasetFilesSaver(\n",
    "        dev_files_wrapper,\n",
    "        max_context_length,\n",
    "        max_question_length,\n",
    "        self.vocab,\n",
    "        dev_raw_data)\n",
    "dev_file_saver.save()\n",
    "\n",
    "print(\"Finished creating training data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iqab-tf14",
   "language": "python",
   "name": "iqab-tf14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
