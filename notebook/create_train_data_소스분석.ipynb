{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import preprocessing.constants as constants\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "from preprocessing.dataset_files_saver import *\n",
    "from preprocessing.dataset_files_wrapper import *\n",
    "from preprocessing.file_util import *\n",
    "from preprocessing.raw_training_data import *\n",
    "from preprocessing.spacy_util import create_tokenizer\n",
    "from preprocessing.string_category import *\n",
    "from preprocessing.vocab import get_vocab\n",
    "from util.string_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BOS = \"bos\"\n",
    "_EOS = \"eos\"\n",
    "\n",
    "_DEBUG_USE_ONLY_FIRST_ARTICLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Some of the training/dev data seems to be inaccurate. This code\n",
    "# tries to make sure that at least one of the \"qa\" options in the acceptable\n",
    "# answers list is accurate and includes it in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPosition:\n",
    "    def __init__(self, start_idx, end_idx):\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageContext:\n",
    "    '''Class used to save the tokenization positions in a given passage\n",
    "       so that the original strings can be used for constructing answer\n",
    "       spans rather than joining tokenized strings, which isn't 100% correct.\n",
    "    '''\n",
    "    def __init__(self, passage_str, word_id_to_text_positions,\n",
    "        acceptable_gnd_truths):\n",
    "        self.passage_str = passage_str\n",
    "        self.word_id_to_text_positions = word_id_to_text_positions\n",
    "        self.acceptable_gnd_truths = acceptable_gnd_truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data, Dev Data 생성의 결과물을 저장할 파일들 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "download_dir = \"../downloads\"\n",
    "value_idx = 0\n",
    "question_id = 0\n",
    "ner_categories = StringCategory()\n",
    "pos_categories = StringCategory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.TRAIN_FOLDER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.DEV_FOLDER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = os.path.join(data_dir, constants.TRAIN_FOLDER_NAME)\n",
    "dev_folder = os.path.join(data_dir, constants.DEV_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question.%d.npy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context.%d.npy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.CONTEXT_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'span.%d.npy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.SPAN_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_in_question.%d.npy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.WORD_IN_QUESTION_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_in_context.%d.npy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.WORD_IN_CONTEXT_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question_ids.%d.npy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question_ids_to_gnd_truths.%d'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_TO_GND_TRUTHS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context.pos.%d.npy'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.CONTEXT_POS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question.pos.%d.npy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_POS_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context.ner.%d.npy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.CONTEXT_NER_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question.ner.%d.npy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_NER_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question_ids_to_squad_question_id.%d'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_TO_SQUAD_QUESTION_ID_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'passage_context.%d'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.QUESTION_IDS_TO_PASSAGE_CONTEXT_FILE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & dev data already exist.\n"
     ]
    }
   ],
   "source": [
    "train_files_wrapper = DatasetFilesWrapper(train_folder)\n",
    "dev_files_wrapper = DatasetFilesWrapper(dev_folder)\n",
    "\n",
    "if all([len(os.listdir(f)) > 0 for f in [train_folder, dev_folder]]):\n",
    "    print(\"Train & dev data already exist.\")\n",
    "    #return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocabulary\n",
      "Vocab size: 2196016\n",
      "Finished getting vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting vocabulary\")\n",
    "vocab = get_vocab(data_dir)\n",
    "print(\"Finished getting vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line : ,\n",
      "\n",
      "line -1 : ,\n",
      "line : .\n",
      "\n",
      "line -1 : .\n",
      "line : the\n",
      "\n",
      "line -1 : the\n",
      "line : and\n",
      "\n",
      "line -1 : and\n",
      "line : to\n",
      "\n",
      "line -1 : to\n",
      "line : of\n",
      "\n",
      "line -1 : of\n",
      "line : a\n",
      "\n",
      "line -1 : a\n",
      "line : in\n",
      "\n",
      "line -1 : in\n",
      "line : \"\n",
      "\n",
      "line -1 : \"\n",
      "line : :\n",
      "\n",
      "line -1 : :\n",
      "Vocab size: 2196016\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "position_to_word = {}\n",
    "word_to_position = {}\n",
    "i = 0\n",
    "with open(os.path.join(data_dir, constants.VOCAB_FILE), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word = line[:-1]\n",
    "        if(i<10):\n",
    "            print(\"line : \" + line)\n",
    "            print(\"line -1 : \" + line[:-1])\n",
    "        word_to_position[word] = i\n",
    "        i += 1\n",
    "position_to_word = {i:word for word, i in word_to_position.items()}\n",
    "print(\"Vocab size: \" + str(len(word_to_position)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Isbill': 2093488,\n",
       " 'Iolaus': 150438,\n",
       " 'AHB': 225529,\n",
       " 'ORRC': 1422066,\n",
       " '11,891': 894370,\n",
       " 'TNKS': 2186871,\n",
       " 'Nixle': 402973,\n",
       " 'archtecture': 1073013,\n",
       " 'Nationally': 47903,\n",
       " '20:22:04': 1695410,\n",
       " 'Pantelic': 803722,\n",
       " '0-150mm': 1872914,\n",
       " 'CMPO': 1672541,\n",
       " 'Euroyen': 2160287,\n",
       " 'FREETEXT': 2102089,\n",
       " 'hat-tip': 403721,\n",
       " 'OutlookFIX': 1850861,\n",
       " 'akaloiolaka6': 1418709,\n",
       " '8,2010': 682263,\n",
       " '16,627': 1171422,\n",
       " 'Fisco': 669329,\n",
       " 'preconscious': 590374,\n",
       " 'imotion': 1852104,\n",
       " 'Aeryn': 142402,\n",
       " '17:19:23': 1561343,\n",
       " 'day/per': 1366856,\n",
       " 'SW1A': 187854,\n",
       " '55491': 2166277,\n",
       " 'Kokinda': 2097886,\n",
       " 'FEAP': 1609336,\n",
       " 'SHXT': 2076644,\n",
       " '2/26/2009': 306655,\n",
       " 'baaad': 314361,\n",
       " 'Eatig': 1043065,\n",
       " 'pre-write': 969558,\n",
       " 'Multo': 1514913,\n",
       " '56,750': 1908083,\n",
       " 'oasis-like': 1343601,\n",
       " 'RAND_MAX': 940598,\n",
       " '˜What': 2010332,\n",
       " 'AndronicusTroilus': 693822,\n",
       " 'Phragmipedium': 1014819,\n",
       " 'Wahyudi': 862534,\n",
       " '65-percent': 1051287,\n",
       " '12:21:36': 978872,\n",
       " 'UsesJust': 2069120,\n",
       " 'Almanya': 920293,\n",
       " '11.04.2008': 895886,\n",
       " 'ALBRIGHT': 331199,\n",
       " 'BidRobot': 2065460,\n",
       " '24568': 1634163,\n",
       " 'Bes': 155158,\n",
       " 'madson': 1214934,\n",
       " 'brick-front': 1914093,\n",
       " 'Bercovitch': 883610,\n",
       " 'day.Most': 2019204,\n",
       " 'painti': 1035478,\n",
       " 'Uncu': 1531572,\n",
       " 'ear-phones': 1165109,\n",
       " 'theposition': 1153881,\n",
       " 'CreateWindow': 780619,\n",
       " 'Article25': 1972045,\n",
       " 'Aelia': 461125,\n",
       " 'EuroSort': 321295,\n",
       " '13:46:26': 1736289,\n",
       " 'Esquivias': 1179513,\n",
       " 'Shelton': 23404,\n",
       " 'Non-Corporate': 648469,\n",
       " 'one.Thank': 1300983,\n",
       " 'moviesmature': 1789115,\n",
       " 'SIVmac251': 2033625,\n",
       " '20.58': 325682,\n",
       " 'sicuramente': 455094,\n",
       " 'Aliss': 1028093,\n",
       " 'inAmerica': 1143088,\n",
       " 'benefit-cost': 387309,\n",
       " '12/carton': 1954254,\n",
       " 'hypobaric': 801948,\n",
       " 'Partilha': 1427078,\n",
       " 'A.\\xa0\\xa0': 723048,\n",
       " 'Clio': 53838,\n",
       " '58-5': 2178747,\n",
       " 'Walburga': 532362,\n",
       " 'Noibai': 1198829,\n",
       " 'sejenis': 1939583,\n",
       " 'prepon': 589531,\n",
       " '1-position': 1403537,\n",
       " 'Dainius': 212881,\n",
       " 'MakePageList': 1367374,\n",
       " 'worksContact': 1676505,\n",
       " '01:48:25': 1477663,\n",
       " '8769': 341040,\n",
       " 'Rivertowns': 206562,\n",
       " 'Jannat': 185974,\n",
       " 'Broto': 693172,\n",
       " 'Intermediate-Term': 664165,\n",
       " '4067': 207866,\n",
       " '19:21:49': 1809106,\n",
       " 'Torelli': 307536,\n",
       " 'Giubileo': 1431384,\n",
       " 'Hrbek': 399411,\n",
       " 'zodiaccity': 451732,\n",
       " 'Arivaca': 306007,\n",
       " 'Spontini': 987961,\n",
       " '18.875': 1542107,\n",
       " 'NOTABLE': 264617,\n",
       " 'ALTO': 104253,\n",
       " 'Lionshare': 1593686,\n",
       " 'turits': 1674941,\n",
       " 'NewsUniversity': 1780085,\n",
       " '3-wheel': 400531,\n",
       " 'Parliaments': 118415,\n",
       " 'taskmanager': 478713,\n",
       " 'Msx2': 1891684,\n",
       " 'RJV': 1561548,\n",
       " 'Sugerman': 478247,\n",
       " 'Phytosterol': 1171138,\n",
       " 'Caveman': 76331,\n",
       " 'ultimativen': 1262776,\n",
       " '12-03-04': 1687651,\n",
       " 'SURVIVES': 636554,\n",
       " 'OMPI': 86165,\n",
       " 'C205': 903322,\n",
       " 'neid': 977511,\n",
       " 'funkey': 960085,\n",
       " \"L'Étoile\": 1992994,\n",
       " 'Norrell': 236729,\n",
       " 'vim': 67521,\n",
       " '10:37:24': 908578,\n",
       " 'thelittlefashionbox': 1336148,\n",
       " 'lusi': 1021304,\n",
       " 'Jugglin': 1253600,\n",
       " 'appauling': 416645,\n",
       " 'Pre-med': 630396,\n",
       " 'MacThomas': 2184433,\n",
       " 'Fitocracy': 602320,\n",
       " 'ReviewsBe': 174229,\n",
       " 'folks!Our': 908427,\n",
       " 'Chuang-Tzu': 1873585,\n",
       " 'textiles': 24914,\n",
       " 'extranjero': 547476,\n",
       " 'dangering': 1967453,\n",
       " '1915-20': 1376995,\n",
       " 'FeedsMobileHot': 2076459,\n",
       " 'Pruszcz': 1885453,\n",
       " 'Cinnamint': 1525938,\n",
       " '27/09/2011': 365207,\n",
       " 'nicley': 517704,\n",
       " 'Colorsound': 1324227,\n",
       " '42404': 1937924,\n",
       " 'Emord': 852693,\n",
       " 'UltraPixel': 1429594,\n",
       " 'Kunstgeschichte': 1341323,\n",
       " 'Lunfardo': 1653736,\n",
       " 'Racialism': 1213867,\n",
       " 'fierce': 14919,\n",
       " 'pmSouth': 1759639,\n",
       " 'Pead': 280631,\n",
       " '80-95': 401942,\n",
       " '-4533': 254502,\n",
       " 'Joiarib': 1927399,\n",
       " 'hexadiene': 2085179,\n",
       " '13Game': 1775226,\n",
       " 'Hueneme': 132265,\n",
       " 'Mindlink': 1156009,\n",
       " 'soph': 190799,\n",
       " 'Thrombosed': 444713,\n",
       " 'Nakajima': 112819,\n",
       " 'Crash/Ride': 1078499,\n",
       " 'glycoalkaloids': 1270099,\n",
       " 'Toquero': 1285421,\n",
       " '21:34:44': 1736860,\n",
       " 'I-575': 664321,\n",
       " 'Omps': 2124312,\n",
       " 'CooperKatz': 2111547,\n",
       " 'multi-surface': 448861,\n",
       " '6930': 197650,\n",
       " 'anti-depressive': 937475,\n",
       " 'Illiquid': 796881,\n",
       " 'High-def': 649162,\n",
       " 'you?That': 1222193,\n",
       " '379-391': 2104864,\n",
       " 'Mosaica': 556948,\n",
       " 'available.Show': 114214,\n",
       " 'Clopra': 2145723,\n",
       " 'SS700': 1884918,\n",
       " '13818-2': 1832414,\n",
       " '.19.2011': 2020011,\n",
       " 'SPRO': 449992,\n",
       " 'Baland': 1700691,\n",
       " '05-18-2001': 1351883,\n",
       " 'review5/9/2012': 1791291,\n",
       " 'picassa': 759458,\n",
       " 'RequestData': 2119088,\n",
       " '2/9/2008': 457545,\n",
       " '17:30:41': 1667811,\n",
       " 'social_networking': 1313659,\n",
       " 'NFC-enabled': 294826,\n",
       " '7/12/00': 1257531,\n",
       " 'Zeilinger': 486396,\n",
       " '303s': 1223010,\n",
       " '62079': 2031350,\n",
       " 'Rank7': 1135703,\n",
       " 'RecipesHoliday': 992850,\n",
       " 'scallopine': 1232923,\n",
       " 'Malthouse': 206313,\n",
       " 'wasreally': 1852187,\n",
       " 'londonoffices.com': 1823272,\n",
       " 'Sanpo': 1594196,\n",
       " 'Maysun': 1933384,\n",
       " 'air-suspension': 1240151,\n",
       " 'pcs/lots': 1833888,\n",
       " 'Mangore': 1249587,\n",
       " 'catalogers': 267538,\n",
       " 'stockcars': 1429061,\n",
       " 'Windwood': 353059,\n",
       " 'Liberica': 1915058,\n",
       " 'INTEGRAL': 188523,\n",
       " 'Sterlington': 356558,\n",
       " 'salinity': 56899,\n",
       " 'RAPPA': 1367941,\n",
       " 'Piravom': 1875992,\n",
       " 'coating': 8663,\n",
       " 'late03': 1919920,\n",
       " 'B05B': 1932065,\n",
       " 'Kaliedascope': 1298424,\n",
       " 'She-Ra': 162359,\n",
       " 'Mladjenovic': 1754784,\n",
       " 'Slaughterhouse': 87671,\n",
       " 'A3F': 1189156,\n",
       " '197130': 1760254,\n",
       " 'pre-agreement': 1632439,\n",
       " 'prinny': 832241,\n",
       " '421-424': 1474582,\n",
       " 'Drsss': 1768658,\n",
       " 'Chivalry': 109625,\n",
       " '386.2': 1556736,\n",
       " '77090': 396939,\n",
       " '21:18:22': 1714986,\n",
       " '6,50': 367265,\n",
       " '13:33:22': 1619690,\n",
       " 'Eurotex': 2135390,\n",
       " 'triality': 2010909,\n",
       " 'SSH2': 267854,\n",
       " 'atnt': 1498984,\n",
       " 'Backout': 929156,\n",
       " 'Shizuma': 624401,\n",
       " 'rmed': 749186,\n",
       " 'Feicheng': 2044280,\n",
       " 'handcoded': 1452044,\n",
       " 'Ambra': 198713,\n",
       " 'Aetolia': 504161,\n",
       " 'Trim.Mercury': 1274792,\n",
       " '23,378': 1507558,\n",
       " 'machinewill': 458379,\n",
       " 'Strominger': 1044242,\n",
       " '18:18:34': 1643564,\n",
       " 'RPG-style': 804910,\n",
       " 'oceanliner': 1312631,\n",
       " '7775': 250720,\n",
       " '11:23:13': 940805,\n",
       " 'common.h': 729334,\n",
       " 'Nathaniel': 26672,\n",
       " '16:11:47': 1500201,\n",
       " '24,744': 1593520,\n",
       " 'I3D': 1136194,\n",
       " 'Hunterz': 951455,\n",
       " 'PDXLAN': 1536724,\n",
       " 'Koos': 192429,\n",
       " 'Ph34r': 1898587,\n",
       " 'riddlers': 1958418,\n",
       " 'Darling': 19404,\n",
       " 'Inadequately': 591808,\n",
       " 'parst': 1613333,\n",
       " 'priority_queue': 1300285,\n",
       " '02:28:14': 1584083,\n",
       " 'heldover': 1530080,\n",
       " 'renny': 768780,\n",
       " 'Banquest': 1894410,\n",
       " 'Shamari': 874862,\n",
       " 'Sgh': 493585,\n",
       " 'diplome': 1055879,\n",
       " 'INGLISH': 1825692,\n",
       " '17:29:34': 1702144,\n",
       " 'DigiFish': 1960505,\n",
       " '1200h': 2121645,\n",
       " 'bothwell': 1239935,\n",
       " 'knighty': 1824896,\n",
       " 'KIER': 1227658,\n",
       " 'chanichim': 1989331,\n",
       " 'thandai': 1887946,\n",
       " 'Kehat': 2002120,\n",
       " '6,156': 524627,\n",
       " 'Takita': 631367,\n",
       " 'imidazole': 216087,\n",
       " 'views27': 951130,\n",
       " 'Guinier': 497498,\n",
       " 'pulminary': 1305258,\n",
       " '09:22:18': 1326839,\n",
       " 'Gwartney': 555407,\n",
       " 'postino': 1373790,\n",
       " 'Stross': 153574,\n",
       " 'Tokyngton': 1438147,\n",
       " 'invita': 355873,\n",
       " 'Descaling': 508512,\n",
       " '1bh': 2063976,\n",
       " 'ughhhhhhh': 1660101,\n",
       " 'Kakes': 503437,\n",
       " 'Stelo': 2085042,\n",
       " 'hokiesports.com': 1435654,\n",
       " 'tentlike': 1906811,\n",
       " 'Cilt': 533141,\n",
       " 'pureview': 897491,\n",
       " '13487': 1005050,\n",
       " 'Paranoia': 76700,\n",
       " 'Monnier': 171379,\n",
       " 'OutlookA': 1219442,\n",
       " 'freeview': 113291,\n",
       " 'E.K.O': 2049724,\n",
       " 'StreetLouisville': 1496421,\n",
       " '22:52:21': 1841466,\n",
       " 'SELLOUT': 584084,\n",
       " 'fundage': 921631,\n",
       " '22,417': 1629661,\n",
       " 'BZN': 423010,\n",
       " 'Nyando': 1848713,\n",
       " 'Manuscripta': 1313795,\n",
       " ',266': 444599,\n",
       " 'Form-fitted': 1720896,\n",
       " 'closed-ear': 1812144,\n",
       " 'Astrometry': 574760,\n",
       " 'Rivershore': 720425,\n",
       " '16,950': 565231,\n",
       " 'MacWarehouse': 1238951,\n",
       " 'Vme': 1011367,\n",
       " '19How': 2011608,\n",
       " 'eatten': 830936,\n",
       " 'UKIDSS': 1495070,\n",
       " 'Hoddy': 939129,\n",
       " 'faast': 1476202,\n",
       " '.3100': 850942,\n",
       " 'Spainards': 1508497,\n",
       " 'expediting': 113714,\n",
       " 'nM': 93075,\n",
       " 'Cumshkt': 1543026,\n",
       " 'Endemol': 198099,\n",
       " '455.99': 1166672,\n",
       " 'Prepubertal': 1048080,\n",
       " 'zzzzzzz': 443428,\n",
       " '42:32': 1258248,\n",
       " 'wurks': 2003797,\n",
       " 'Taïwan': 1545578,\n",
       " 'Yoooo': 718380,\n",
       " 're-charter': 1517300,\n",
       " 'Phonecard': 572257,\n",
       " 'prochoice': 418318,\n",
       " 'Ithorians': 1854614,\n",
       " '16.8': 61277,\n",
       " '1923-2007': 1782422,\n",
       " 'AdoramaPix': 1087540,\n",
       " 'StorageMojo': 2072439,\n",
       " '10:53:58': 968675,\n",
       " 'AgribusinessHealthcare': 1853974,\n",
       " 'Sloans': 381483,\n",
       " 'santiclaws': 1112226,\n",
       " 'Funkoars': 1171465,\n",
       " '06:23:53': 1652509,\n",
       " 'blueteen': 1507391,\n",
       " 'caliche': 444045,\n",
       " '5-lipoxygenase': 490215,\n",
       " '23:43:56': 1927198,\n",
       " 'previe': 910020,\n",
       " 'Hoopstad': 1996186,\n",
       " 'sl4': 273181,\n",
       " 'TransactionManager': 974341,\n",
       " 'Furtney': 1289215,\n",
       " '03/31/08': 508171,\n",
       " '8HQ': 604196,\n",
       " 'unusually': 20127,\n",
       " 'mayonnaise': 31796,\n",
       " 'Privatsphäre': 1141105,\n",
       " '26039': 1486632,\n",
       " '15:31:33': 1575488,\n",
       " 'WGA': 73436,\n",
       " 'hideabed': 1886155,\n",
       " 'VJO': 2174503,\n",
       " 'proprety': 1144295,\n",
       " 'Blogmas': 1940961,\n",
       " 'Breitbach': 821485,\n",
       " 'Chope': 504980,\n",
       " 'Hüllen': 1627808,\n",
       " 'Spenst': 2093633,\n",
       " 'Kuta': 65090,\n",
       " '16:58:08': 1507532,\n",
       " '423.4': 1762872,\n",
       " 'Buriza': 2190548,\n",
       " 'Mitchellsburg': 1213491,\n",
       " '98101': 155690,\n",
       " '200000.00': 2156961,\n",
       " 'HAULAGE': 725278,\n",
       " '32266': 951027,\n",
       " '7,415': 622277,\n",
       " 'angelic_one2002': 561345,\n",
       " 'say.Benghazi': 1294250,\n",
       " 'muzikale': 555669,\n",
       " 'Brignac': 220186,\n",
       " 'preterminated': 2090980,\n",
       " 'VIRDEN': 2150503,\n",
       " 'helal': 300655,\n",
       " 'Namoro': 1172616,\n",
       " 'Xbalanque': 1563345,\n",
       " 'isn?t': 159413,\n",
       " '11-10-18': 2143343,\n",
       " 'sol26-sparc-local': 1399286,\n",
       " 'STPNS': 1804698,\n",
       " '60-98': 1886943,\n",
       " 'Sleeps': 18028,\n",
       " 'Sechura': 1753320,\n",
       " 'Mailday': 508930,\n",
       " 'represnt': 1237367,\n",
       " '14:40:20': 1525084,\n",
       " 'getGroups': 1977515,\n",
       " '10:46': 21896,\n",
       " 'stupendously': 214997,\n",
       " 'U.S.C.': 14922,\n",
       " '2,685': 255479,\n",
       " '20:22:43': 1781443,\n",
       " 'Speelduur': 699428,\n",
       " 'deridder': 1423728,\n",
       " 'permalinksavepa': 982957,\n",
       " 'Arkaos': 787743,\n",
       " 'Bexhill-On-Sea': 583841,\n",
       " 'Ajoka': 859777,\n",
       " '05:30:40': 1728945,\n",
       " '28.96': 330492,\n",
       " 'alpha_s': 2190128,\n",
       " 'illocutionary': 576123,\n",
       " 'T-Nuts': 1182218,\n",
       " 'Hectorite': 1892217,\n",
       " 'FORECASTER': 788842,\n",
       " 'Lemenager': 1433664,\n",
       " 'hairsticks': 1327043,\n",
       " 'KashFlow': 735919,\n",
       " 'punched-card': 1713315,\n",
       " 'KOOL-AID': 536991,\n",
       " 'compubox': 2050018,\n",
       " '2Starting': 951944,\n",
       " 'Htoo': 510293,\n",
       " 'naw': 80630,\n",
       " 'Neutral': 18573,\n",
       " 'Diadem': 248692,\n",
       " 'guaranteed.Minimum': 1078587,\n",
       " 'Bipp': 447638,\n",
       " 'Narvaez': 220835,\n",
       " 'ToonDownSouth': 2173737,\n",
       " '19:11:26': 1769542,\n",
       " 'Scoopful': 2049219,\n",
       " '+149': 555673,\n",
       " 'BARRICHELLO': 2056550,\n",
       " 'YingYang': 2033702,\n",
       " 'Turnock': 762138,\n",
       " 'Fluoresce': 1624362,\n",
       " 'medow': 1005595,\n",
       " \"d'Epinay\": 1570872,\n",
       " 'Jackeline': 510937,\n",
       " 'Høiby': 1930954,\n",
       " 'befouled': 515376,\n",
       " 'www.kfoxtv.com': 558239,\n",
       " 'Hume-Fogg': 1236390,\n",
       " '28,423': 1992798,\n",
       " 'USLT': 1370792,\n",
       " '06:50:47': 1521276,\n",
       " 'Nazila': 959897,\n",
       " 'Cchetty': 1068975,\n",
       " 'Esplen': 1191884,\n",
       " 'KUBA': 803649,\n",
       " 'Weeeeell': 1923350,\n",
       " 'exx': 929096,\n",
       " 'S02e07': 2050583,\n",
       " 'soy-unapersonadesagradable': 1059381,\n",
       " 'chlorox': 1518009,\n",
       " 'Deregulatory': 1754716,\n",
       " 'odaibacasinos': 1104468,\n",
       " 'SP/LP': 1537479,\n",
       " 'tragics': 641593,\n",
       " '2:09:26': 2192005,\n",
       " ':457': 378289,\n",
       " 'Tashilhunpo': 901962,\n",
       " 'HKD$': 572214,\n",
       " 'Pervenche': 1920913,\n",
       " 'prash': 1579550,\n",
       " '2007â': 2079772,\n",
       " '49:8': 695436,\n",
       " '2012-2016Figure': 1374461,\n",
       " 'Vunak': 843191,\n",
       " 'FERROUS': 623847,\n",
       " 'baseType': 1954058,\n",
       " 'deligence': 1769392,\n",
       " 'NJASK': 1322172,\n",
       " '4866': 244771,\n",
       " 'Fumento': 456465,\n",
       " '-2420': 1822823,\n",
       " '20:46:41': 1738362,\n",
       " 'mastrubate': 640297,\n",
       " 'Earman': 699971,\n",
       " 'GRW': 535302,\n",
       " 'Arctopus': 833680,\n",
       " 'Bookninja': 1166696,\n",
       " 'aqo': 2043867,\n",
       " 'tuned.': 867461,\n",
       " 'BIAW': 583611,\n",
       " 'Carcharhinidae': 2131558,\n",
       " 'Airstrikes': 377221,\n",
       " '600-1200': 832777,\n",
       " '16197': 1314213,\n",
       " 'Orly': 63754,\n",
       " 'Bowdler': 343921,\n",
       " 'DAAA': 1045558,\n",
       " 'Respectful': 114799,\n",
       " 'Transgenomic': 744312,\n",
       " 'Utiel': 1330759,\n",
       " '95467': 1932046,\n",
       " 'vota': 481692,\n",
       " 'Rhamnaceae': 1288063,\n",
       " 'Outfeed': 908747,\n",
       " 'goferphan': 955839,\n",
       " 'BNPP': 693373,\n",
       " 'Spitaleri': 1906621,\n",
       " 'Vandermonde': 1132426,\n",
       " 'microphthalmia': 702993,\n",
       " 'chocolate.Rank': 1516861,\n",
       " 'DEBIAN': 668964,\n",
       " 'Getriebe': 1139551,\n",
       " 'Handi-Foam': 1783626,\n",
       " 'In-Stores': 1390737,\n",
       " 'NOMINATING': 704186,\n",
       " 'sukhbir': 1963289,\n",
       " 'Areta': 921790,\n",
       " '-0865': 256883,\n",
       " 'Ganges-Brahmaputra': 2040232,\n",
       " '22.07.2008': 1031885,\n",
       " 'limewire': 79677,\n",
       " 'cream-pies': 777830,\n",
       " '1,599.99': 362715,\n",
       " 'IQU': 1420787,\n",
       " 'Downsizers': 1996743,\n",
       " 'vfo': 1332419,\n",
       " '14:52:19': 1585042,\n",
       " 'Bifidobacteria': 518325,\n",
       " 'unblock': 66256,\n",
       " 'modern-ish': 1771000,\n",
       " 'Bidwell': 106841,\n",
       " 'i.MX515': 1631900,\n",
       " 'CORBETT': 286180,\n",
       " 'Nonresistance': 1768758,\n",
       " '99.27': 984519,\n",
       " 'SP28': 1837431,\n",
       " '6/24/01': 1291656,\n",
       " 'agoodthinghappened': 1508141,\n",
       " 'Gabrel': 1950129,\n",
       " 'Keldagrim': 1380263,\n",
       " 'MLSID': 1036632,\n",
       " 'nailed-on': 1551098,\n",
       " 'mojis': 1877268,\n",
       " '19-01-2010': 554819,\n",
       " 'Cuiviénen': 1942805,\n",
       " 'CAFollow': 1416190,\n",
       " '5White': 1255298,\n",
       " 'vmode': 2153816,\n",
       " 'PlantsComments': 1085839,\n",
       " 'Teodosije': 2074507,\n",
       " 'Laras': 599959,\n",
       " 'punch': 8110,\n",
       " '47108': 2052385,\n",
       " 'cricketdiane': 1906691,\n",
       " 'grüße': 745749,\n",
       " 'Racecards': 189886,\n",
       " 'V5R1': 1018806,\n",
       " 'Gritten': 1087729,\n",
       " 'kyr': 320328,\n",
       " '23:50:15': 1902858,\n",
       " 'Bottle/Can': 1159890,\n",
       " 'bilum': 2162045,\n",
       " 'Distresser': 1787230,\n",
       " '64.87': 844623,\n",
       " 'owww': 407338,\n",
       " 'OFF25': 1387779,\n",
       " 'servient': 594945,\n",
       " 'roastin': 1190276,\n",
       " 'thereWe': 1873989,\n",
       " 'Drowsey': 1850768,\n",
       " '18-07-2007': 952451,\n",
       " '50.22': 548150,\n",
       " 'TSection': 1989922,\n",
       " 'Xydias': 1593290,\n",
       " 'Homofobia': 2022948,\n",
       " 'McBoing': 1117730,\n",
       " '354.95': 1055791,\n",
       " '2-7-09': 1488381,\n",
       " '07-10-2008': 270975,\n",
       " 'C-mon': 1839361,\n",
       " '79-67': 837826,\n",
       " 'traumatise': 849899,\n",
       " 'snapseed': 1183807,\n",
       " '42-25': 751672,\n",
       " 'Masaba': 672291,\n",
       " '00:18:52': 1758603,\n",
       " '2009India': 1897759,\n",
       " '04:27:55': 1705715,\n",
       " 'Buder': 700783,\n",
       " 'HSC': 59912,\n",
       " '4,010': 340565,\n",
       " 'stonecutters': 687606,\n",
       " '14:33:03': 1574552,\n",
       " 'streetcoyote': 1863573,\n",
       " 'cabildo': 1019842,\n",
       " 'Mineski': 1195627,\n",
       " 'King': 1208,\n",
       " 'e260': 589901,\n",
       " 'welcum': 483344,\n",
       " 'Carrot-Top': 1313222,\n",
       " 'humilating': 721843,\n",
       " 'graviton': 323630,\n",
       " 'lights/high': 420634,\n",
       " 'rel': 33352,\n",
       " 'proti': 692265,\n",
       " 'Supertex': 764410,\n",
       " 'DealsHi': 321690,\n",
       " '-3618': 827626,\n",
       " '[MLive.com': 1891212,\n",
       " 'MiG-23': 669464,\n",
       " 'X820': 791763,\n",
       " 'social-oriented': 2191094,\n",
       " 'GMTGovernor': 1512563,\n",
       " 'H-15': 1088795,\n",
       " 'Yur': 273189,\n",
       " 'vidcaps': 286430,\n",
       " '09:40:21': 1307792,\n",
       " 'SnGs': 591610,\n",
       " 'tax-benefit': 1936674,\n",
       " 'Wahneta': 836710,\n",
       " '11/3/06': 736007,\n",
       " 'Swingstar': 1478232,\n",
       " 'Xiaorong': 1234176,\n",
       " 'pocess': 1024975,\n",
       " 'SAGAN': 778290,\n",
       " 'Aantares': 1786141,\n",
       " 'CRUISES': 182499,\n",
       " '000th': 835513,\n",
       " 'himeno': 2188646,\n",
       " '21,213': 1502721,\n",
       " 'drops': 6125,\n",
       " 'monde-des-hommes': 962997,\n",
       " 'Nuix': 954422,\n",
       " 'casuynos': 1567245,\n",
       " 'days.': 157254,\n",
       " 'haizz': 367344,\n",
       " 'buildworld': 522207,\n",
       " 'Stretch-marks': 2179894,\n",
       " 'tin-mining': 1689570,\n",
       " '176.7': 689881,\n",
       " 'General11': 738877,\n",
       " 'undistinguished': 157183,\n",
       " 'Ramnarine': 653303,\n",
       " 'JISD': 1547190,\n",
       " 'gryulich': 1463955,\n",
       " 'GLAM': 142235,\n",
       " 'EnergyNutrient': 1617371,\n",
       " 'friendly': 1965,\n",
       " 'Mullarney': 1547219,\n",
       " 'OlivePad': 1972218,\n",
       " 'Sassari': 205660,\n",
       " 'horcruxes': 393426,\n",
       " '9386': 331510,\n",
       " 'PC/Server': 1300447,\n",
       " 'Gyong': 1938031,\n",
       " 'Silverlining': 1270966,\n",
       " 'WV1': 350884,\n",
       " 'AMPL': 502786,\n",
       " 'Neverwinter': 64767,\n",
       " 'MISS.': 348843,\n",
       " 'ords': 398544,\n",
       " 'CIA-funded': 1207383,\n",
       " 'Metalite': 1147334,\n",
       " 'TIPP': 516820,\n",
       " 'brackish': 93181,\n",
       " 'http://www.domesticpsychology.com/blog/': 2181600,\n",
       " 'nomas': 367427,\n",
       " 'exellent': 108262,\n",
       " 'Shimbun': 127827,\n",
       " 'Terralta': 2181458,\n",
       " 'STOOP': 905913,\n",
       " 'cxm': 1601361,\n",
       " 'officialmeganandliz': 2184737,\n",
       " 'Viacom18': 1214690,\n",
       " 'Hoveyda': 1112148,\n",
       " 'PMlearn': 1645257,\n",
       " 'needless': 26586,\n",
       " 'Sirat': 481506,\n",
       " 'Uncomplicated': 162099,\n",
       " 'VTCS': 2045826,\n",
       " 'YAHH': 1302648,\n",
       " 'Chi-Ali': 1616405,\n",
       " '69K': 294242,\n",
       " 'w/New': 769430,\n",
       " 'Minestrone': 165076,\n",
       " 'full-round': 479495,\n",
       " 'Server.Execute': 1934626,\n",
       " '04:06:06': 1594830,\n",
       " 'leftM14Silver': 2101757,\n",
       " 'Motosports': 762127,\n",
       " 'pretiest': 1922267,\n",
       " 'Ingelheim': 113847,\n",
       " 'LEVERS': 355182,\n",
       " '8mMore': 725264,\n",
       " 'episodios': 366131,\n",
       " 'writier': 2126925,\n",
       " 'stricte': 1757611,\n",
       " 'deerhunter': 655902,\n",
       " 'shoppy': 1886223,\n",
       " '11/23/2004': 578925,\n",
       " 'stepstools': 2019327,\n",
       " '07:15:04': 1413018,\n",
       " 'blue/purple': 360647,\n",
       " 'US-60': 767404,\n",
       " 'Otr': 482946,\n",
       " '8U4': 2054477,\n",
       " '2-book': 1760270,\n",
       " 'UpdateSubtotal': 178247,\n",
       " 'AutoFormat': 559038,\n",
       " 'bewitchingly': 706830,\n",
       " 'Conetta': 1143995,\n",
       " '#american': 534933,\n",
       " 'sentimentos': 923948,\n",
       " 'WebFilter': 1224329,\n",
       " 'Allee': 121989,\n",
       " '1:11:32': 1756170,\n",
       " 't-nation': 1793412,\n",
       " '400iso': 1776852,\n",
       " 'Anessa': 560015,\n",
       " 'self-neglect': 605823,\n",
       " 'Strothman': 1207460,\n",
       " '#monochrome': 1021056,\n",
       " '6/01/02': 2123303,\n",
       " 'GRB': 121793,\n",
       " 'conscientiousness': 197332,\n",
       " 'LIFE-LIKE': 1197012,\n",
       " 'Sadka': 1031410,\n",
       " 'M4-2': 1818348,\n",
       " '74HC04': 2147865,\n",
       " '4DM1N': 1043319,\n",
       " '26,9605,0': 1636669,\n",
       " 'Scav': 890354,\n",
       " 'q45': 731394,\n",
       " '08/02/2002': 1476311,\n",
       " '13-Nov-2012': 1106272,\n",
       " 'YTP': 203241,\n",
       " 'taxco': 723248,\n",
       " 'Juliá': 1154667,\n",
       " 'PrepStar': 547758,\n",
       " 'Hottopic': 1842110,\n",
       " 'refriger': 1502989,\n",
       " 'w/Mesh': 2102567,\n",
       " 'PICgrabber': 2138471,\n",
       " 'Stower': 775996,\n",
       " 'Hairy': 17331,\n",
       " 'agoPatricia': 923361,\n",
       " '07.12.07': 1255926,\n",
       " 'BT474': 2004588,\n",
       " 'Kanaria': 1561064,\n",
       " 'kB/sec': 1805809,\n",
       " '9812': 354433,\n",
       " 'Levu': 184102,\n",
       " 'Interims': 680234,\n",
       " 'Mienshao': 938887,\n",
       " '-1443': 245936,\n",
       " '5-7yrs': 1495270,\n",
       " '00:53:17': 2131392,\n",
       " 'MARWAN': 1274985,\n",
       " 'summer-run': 2156862,\n",
       " 'Iunno': 894494,\n",
       " 'Uncircumcision': 1299227,\n",
       " 'operands': 97616,\n",
       " '13,456': 975782,\n",
       " 'dugout': 51534,\n",
       " 'MemberTopics': 1991773,\n",
       " 'VEIWS': 1436875,\n",
       " '424206': 1756754,\n",
       " 'RARE': 17371,\n",
       " 'XBoX': 1436886,\n",
       " 'Jennys': 239359,\n",
       " 'Uhlaender': 1107663,\n",
       " '89101112131415161718': 2014979,\n",
       " 'Zumthor': 423492,\n",
       " 'SHIFTING': 250787,\n",
       " 'Sipalay': 839983,\n",
       " 'Impa': 416919,\n",
       " '26,271': 1686073,\n",
       " 'resque': 697289,\n",
       " '07.15.2011': 1810665,\n",
       " 'Hermeneutics': 139151,\n",
       " 'Taoist': 73106,\n",
       " '29-08-08': 2032624,\n",
       " 'M309': 2082134,\n",
       " 'CUSTO': 832267,\n",
       " 'u0435': 1805889,\n",
       " 'Noder': 1654761,\n",
       " 'Torborg': 509345,\n",
       " 'priceSiberian': 2040600,\n",
       " 'LSU': 12786,\n",
       " 'morphometry': 304582,\n",
       " 'data-structure': 786240,\n",
       " '41,961': 2133768,\n",
       " 'Alpha-2': 936151,\n",
       " 'lowest': 4463,\n",
       " 'witness': 6249,\n",
       " 'showin': 112217,\n",
       " 'multi-account': 961902,\n",
       " 'paddywagon': 1805835,\n",
       " 'Postsaver': 1506926,\n",
       " 'A+G': 597674,\n",
       " 'www.led-underground-light.com/PRODUCT/LANG-ro/Led': 794289,\n",
       " 'Soebarkah': 2030274,\n",
       " 'obligor': 135883,\n",
       " 'Boffi': 584227,\n",
       " 'ROSWELL': 260712,\n",
       " 'Sherline': 331903,\n",
       " 'UpdateStar': 407763,\n",
       " 'well-sited': 1739831,\n",
       " 'Cowpeas': 1171457,\n",
       " 'pownce': 783827,\n",
       " 'Rufty': 922788,\n",
       " 'plantarflexion': 883787,\n",
       " 'Silpa': 763624,\n",
       " 'Ivo': 70673,\n",
       " 'Wirra': 623518,\n",
       " 'half-bird': 1353874,\n",
       " '02:25:35': 1559980,\n",
       " 'Complementos': 971014,\n",
       " 'Ghigliotti': 1726554,\n",
       " 'familymwr': 1888397,\n",
       " 'table-based': 389327,\n",
       " 'Barcon': 2176493,\n",
       " 'Groenink': 2142090,\n",
       " 'Chavismo': 609912,\n",
       " 'Kapampangans': 1625842,\n",
       " '650-322': 1734289,\n",
       " 'MFC-8860DN': 1734421,\n",
       " 'coupons.com': 238777,\n",
       " 'NMSZ': 2063579,\n",
       " 'Briann': 513162,\n",
       " 'unitframes': 1468008,\n",
       " 'Kumpfer': 2177432,\n",
       " 'AccountPay': 1200351,\n",
       " 'BC3': 336207,\n",
       " '2,597': 292475,\n",
       " '10396': 586471,\n",
       " 'gila': 149925,\n",
       " 'metaphysical': 34274,\n",
       " 'Exeter-based': 1638268,\n",
       " '106.66': 1318715,\n",
       " '20/08/12': 1175493,\n",
       " '10kms': 150692,\n",
       " '8-count': 740533,\n",
       " 'Nourriture': 1003640,\n",
       " 'she-devil': 466068,\n",
       " 'Tailzz': 1566800,\n",
       " 'Polygala': 760227,\n",
       " 'ETISALAT': 1235314,\n",
       " 'N.P.G.': 2040317,\n",
       " 'self-appointed': 92261,\n",
       " '19:02:58': 1767974,\n",
       " 'Sonnet': 66556,\n",
       " 'JMCC': 1185272,\n",
       " 'HoldersWedding': 504496,\n",
       " 'Brakemen': 1775367,\n",
       " 'edge.The': 766489,\n",
       " 'sympoms': 1471060,\n",
       " 'FanDub': 988751,\n",
       " 'nOvaMatic': 1722655,\n",
       " '11-09-2000': 1737279,\n",
       " 'first-out': 302502,\n",
       " 'Chabela': 1767589,\n",
       " 'anorectal': 268063,\n",
       " '-5.3': 321178,\n",
       " 'PeopleFirst': 1399162,\n",
       " 'Windswept': 195074,\n",
       " 'close-ish': 1190427,\n",
       " 'Elrohir': 183534,\n",
       " 'Spa/hot': 2185283,\n",
       " 'jgraz': 1566413,\n",
       " '1ra': 349314,\n",
       " 'data-structures': 551075,\n",
       " 'Gawi': 2127207,\n",
       " 'hajde': 1333710,\n",
       " 'Karsyn': 965898,\n",
       " 'programms': 231781,\n",
       " '98.82': 848965,\n",
       " '4/3/2002': 851826,\n",
       " 'fanny-pack': 1244323,\n",
       " '28Aug': 2132206,\n",
       " '06-Dec': 2057089,\n",
       " '18-In': 1890258,\n",
       " 'MetadatenAuthor': 1359125,\n",
       " '2013Kim': 581142,\n",
       " 'LIZ': 109951,\n",
       " 'Grammarly': 564785,\n",
       " 'Bulger': 69869,\n",
       " 'DB_CONFIG': 1177794,\n",
       " 'kibbutz': 130399,\n",
       " 'loow': 1649428,\n",
       " '1900s': 37142,\n",
       " 'SHUTOUT': 930775,\n",
       " 'Janaa': 1882010,\n",
       " '2FObama': 1452157,\n",
       " '35,935': 2044797,\n",
       " '4TopPrevious': 1201519,\n",
       " 'amca': 1618028,\n",
       " '40,425': 2104870,\n",
       " 'Duméril': 2086195,\n",
       " 'ARCHIVE': 51496,\n",
       " 'potty-mouthed': 449455,\n",
       " 'location-related': 1078895,\n",
       " 'Feevale': 2159491,\n",
       " 'Mlm': 101336,\n",
       " 'eCPMs': 1593798,\n",
       " 'Bobbys': 468673,\n",
       " 'Suhara': 1992480,\n",
       " 'Dustmann': 1872532,\n",
       " 'Fuckeed': 1073380,\n",
       " 'edX': 423289,\n",
       " 'Notams': 1493537,\n",
       " 'Druse': 404294,\n",
       " 'splash': 13951,\n",
       " 'hecatombs': 1333983,\n",
       " 'Ponder': 43540,\n",
       " 'Calabria': 67771,\n",
       " 'Saitek': 144306,\n",
       " 'TM-T88IV': 866537,\n",
       " 'Litchi': 309387,\n",
       " '02:43:13': 1618155,\n",
       " 'helpful?Read': 263208,\n",
       " 'aquatints': 705506,\n",
       " 'Sossusvlei': 247392,\n",
       " 'Polimeri': 1833707,\n",
       " 'just-introduced': 2002365,\n",
       " 'Arlesheim': 1158498,\n",
       " 'crash': 4575,\n",
       " 'interfertile': 2059041,\n",
       " '11081': 563609,\n",
       " 'Ifuckedherfinally': 1168561,\n",
       " 'Deuker': 1319811,\n",
       " 'DOOR-TO-DOOR': 1824611,\n",
       " 'SERGE': 372136,\n",
       " 'Guadalupe-Blanco': 2068937,\n",
       " '01517': 1359837,\n",
       " 'Kulakowski': 1355509,\n",
       " 'offprint': 464175,\n",
       " '10-31': 321024,\n",
       " 'EVENTZ': 2057292,\n",
       " 'Menowin': 1311014,\n",
       " 'all.Quote': 2178407,\n",
       " 'Monsoonal': 1774941,\n",
       " '60062': 300296,\n",
       " '130.95': 518299,\n",
       " 'Parole': 55820,\n",
       " 'Baldwinsville': 151632,\n",
       " 'INGENIEUR': 1443943,\n",
       " '4NB': 543673,\n",
       " 'Popof': 489336,\n",
       " '17:58:26': 1745712,\n",
       " 'called.I': 1699342,\n",
       " 'RT-3': 1765647,\n",
       " 'économie': 435396,\n",
       " 'Riverlife': 1152053,\n",
       " 'doorknobs': 150004,\n",
       " '2.161': 1662199,\n",
       " 'SMWF': 2186828,\n",
       " 'Heatherington': 445231,\n",
       " 'Bonany': 1986468,\n",
       " 'Nahimana': 1748061,\n",
       " 'MomBirthday': 733099,\n",
       " 'J-Roc': 1005081,\n",
       " 'appropo': 860129,\n",
       " '09:01:01': 1183234,\n",
       " 'Akay666': 804438,\n",
       " 'Opsvik': 1033804,\n",
       " 'Doridro': 2007919,\n",
       " 'Mirrer': 958295,\n",
       " '08/31/04': 691339,\n",
       " 'applications/1x1': 546710,\n",
       " 'Labcoats': 1360819,\n",
       " 'dataRow': 1781761,\n",
       " '2008/06/14': 1419457,\n",
       " '08-Jul-11': 1774696,\n",
       " 'TypeLED': 1103940,\n",
       " 'non-persistent': 377096,\n",
       " 'Tentatively': 191091,\n",
       " '2002-04-27': 2026626,\n",
       " 'WHISPERER': 422926,\n",
       " ...}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'the',\n",
       " 3: 'and',\n",
       " 4: 'to',\n",
       " 5: 'of',\n",
       " 6: 'a',\n",
       " 7: 'in',\n",
       " 8: '\"',\n",
       " 9: ':',\n",
       " 10: 'is',\n",
       " 11: 'for',\n",
       " 12: 'I',\n",
       " 13: ')',\n",
       " 14: '(',\n",
       " 15: 'that',\n",
       " 16: '-',\n",
       " 17: 'on',\n",
       " 18: 'you',\n",
       " 19: 'with',\n",
       " 20: \"'s\",\n",
       " 21: 'it',\n",
       " 22: 'The',\n",
       " 23: 'are',\n",
       " 24: 'by',\n",
       " 25: 'at',\n",
       " 26: 'be',\n",
       " 27: 'this',\n",
       " 28: 'as',\n",
       " 29: 'from',\n",
       " 30: 'was',\n",
       " 31: 'have',\n",
       " 32: 'or',\n",
       " 33: '...',\n",
       " 34: 'your',\n",
       " 35: 'not',\n",
       " 36: '!',\n",
       " 37: '?',\n",
       " 38: 'will',\n",
       " 39: 'an',\n",
       " 40: \"n't\",\n",
       " 41: 'can',\n",
       " 42: 'but',\n",
       " 43: 'all',\n",
       " 44: 'my',\n",
       " 45: 'has',\n",
       " 46: '|',\n",
       " 47: 'do',\n",
       " 48: 'we',\n",
       " 49: 'they',\n",
       " 50: 'more',\n",
       " 51: 'one',\n",
       " 52: 'about',\n",
       " 53: 'he',\n",
       " 54: ';',\n",
       " 55: \"'\",\n",
       " 56: 'out',\n",
       " 57: '$',\n",
       " 58: 'their',\n",
       " 59: 'so',\n",
       " 60: 'his',\n",
       " 61: 'up',\n",
       " 62: 'It',\n",
       " 63: '&',\n",
       " 64: 'like',\n",
       " 65: '/',\n",
       " 66: '1',\n",
       " 67: 'which',\n",
       " 68: 'if',\n",
       " 69: 'would',\n",
       " 70: 'our',\n",
       " 71: '[',\n",
       " 72: ']',\n",
       " 73: 'me',\n",
       " 74: 'who',\n",
       " 75: 'just',\n",
       " 76: 'This',\n",
       " 77: 'time',\n",
       " 78: 'what',\n",
       " 79: 'A',\n",
       " 80: '2',\n",
       " 81: 'had',\n",
       " 82: 'when',\n",
       " 83: 'there',\n",
       " 84: 'been',\n",
       " 85: 'some',\n",
       " 86: 'get',\n",
       " 87: 'were',\n",
       " 88: 'other',\n",
       " 89: 'also',\n",
       " 90: 'In',\n",
       " 91: 'her',\n",
       " 92: 'them',\n",
       " 93: 'You',\n",
       " 94: 'new',\n",
       " 95: 'We',\n",
       " 96: 'no',\n",
       " 97: 'any',\n",
       " 98: '>',\n",
       " 99: 'people',\n",
       " 100: 'than',\n",
       " 101: 'into',\n",
       " 102: 'only',\n",
       " 103: '3',\n",
       " 104: 'how',\n",
       " 105: 'its',\n",
       " 106: 'first',\n",
       " 107: 'said',\n",
       " 108: 'i',\n",
       " 109: 'If',\n",
       " 110: 'over',\n",
       " 111: 'make',\n",
       " 112: 'good',\n",
       " 113: 'know',\n",
       " 114: 'very',\n",
       " 115: '%',\n",
       " 116: 'am',\n",
       " 117: 'now',\n",
       " 118: 'see',\n",
       " 119: 'may',\n",
       " 120: 'she',\n",
       " 121: 'could',\n",
       " 122: 'most',\n",
       " 123: 'then',\n",
       " 124: \"'m\",\n",
       " 125: 'use',\n",
       " 126: 'these',\n",
       " 127: 'did',\n",
       " 128: 'And',\n",
       " 129: 'way',\n",
       " 130: 'New',\n",
       " 131: '4',\n",
       " 132: 'here',\n",
       " 133: 'well',\n",
       " 134: 'work',\n",
       " 135: 'two',\n",
       " 136: '=',\n",
       " 137: 'think',\n",
       " 138: 'much',\n",
       " 139: 'should',\n",
       " 140: 'us',\n",
       " 141: 'years',\n",
       " 142: 'many',\n",
       " 143: 'back',\n",
       " 144: '2012',\n",
       " 145: 'because',\n",
       " 146: '5',\n",
       " 147: '#',\n",
       " 148: 'For',\n",
       " 149: 'after',\n",
       " 150: 'He',\n",
       " 151: 'need',\n",
       " 152: 'year',\n",
       " 153: 'even',\n",
       " 154: 'does',\n",
       " 155: 'really',\n",
       " 156: 'through',\n",
       " 157: 'want',\n",
       " 158: 'great',\n",
       " 159: 'him',\n",
       " 160: 'PM',\n",
       " 161: 'where',\n",
       " 162: 'go',\n",
       " 163: '2011',\n",
       " 164: 'day',\n",
       " 165: '--',\n",
       " 166: 'such',\n",
       " 167: '10',\n",
       " 168: '*',\n",
       " 169: 'right',\n",
       " 170: 'best',\n",
       " 171: 'made',\n",
       " 172: \"'re\",\n",
       " 173: 'views',\n",
       " 174: 'But',\n",
       " 175: 'find',\n",
       " 176: 'take',\n",
       " 177: 'being',\n",
       " 178: \"'ve\",\n",
       " 179: 'those',\n",
       " 180: 'information',\n",
       " 181: '2010',\n",
       " 182: 'before',\n",
       " 183: 'used',\n",
       " 184: 'off',\n",
       " 185: 'love',\n",
       " 186: 'home',\n",
       " 187: 'too',\n",
       " 188: '+',\n",
       " 189: 'last',\n",
       " 190: 'What',\n",
       " 191: 'going',\n",
       " 192: 'help',\n",
       " 193: 'To',\n",
       " 194: 'still',\n",
       " 195: 'free',\n",
       " 196: 'down',\n",
       " 197: 'There',\n",
       " 198: 'All',\n",
       " 199: 'May',\n",
       " 200: '2009',\n",
       " 201: 'They',\n",
       " 202: '6',\n",
       " 203: 'life',\n",
       " 204: 'same',\n",
       " 205: 'around',\n",
       " 206: \"'ll\",\n",
       " 207: 'world',\n",
       " 208: 'My',\n",
       " 209: 'pm',\n",
       " 210: 'own',\n",
       " 211: 'little',\n",
       " 212: 'while',\n",
       " 213: 'each',\n",
       " 214: 'look',\n",
       " 215: 'By',\n",
       " 216: 'say',\n",
       " 217: 'AM',\n",
       " 218: 'got',\n",
       " 219: 'So',\n",
       " 220: 'business',\n",
       " 221: '7',\n",
       " 222: 'As',\n",
       " 223: '0',\n",
       " 224: 'That',\n",
       " 225: '.....',\n",
       " 226: 'part',\n",
       " 227: 'both',\n",
       " 228: 'long',\n",
       " 229: 'better',\n",
       " 230: 'every',\n",
       " 231: 'Home',\n",
       " 232: 'few',\n",
       " 233: 'ago',\n",
       " 234: 'things',\n",
       " 235: 'between',\n",
       " 236: '8',\n",
       " 237: 'Posted',\n",
       " 238: 'something',\n",
       " 239: 'game',\n",
       " 240: '2008',\n",
       " 241: 'With',\n",
       " 242: 'No',\n",
       " 243: 'available',\n",
       " 244: 'never',\n",
       " 245: 'using',\n",
       " 246: 'place',\n",
       " 247: 'under',\n",
       " 248: 'come',\n",
       " 249: 'site',\n",
       " 250: '12',\n",
       " 251: 'online',\n",
       " 252: 'another',\n",
       " 253: 'system',\n",
       " 254: 'found',\n",
       " 255: 'How',\n",
       " 256: 'When',\n",
       " 257: 'days',\n",
       " 258: 'without',\n",
       " 259: 'always',\n",
       " 260: 'On',\n",
       " 261: 'different',\n",
       " 262: '20',\n",
       " 263: 'next',\n",
       " 264: 'set',\n",
       " 265: 'must',\n",
       " 266: 'since',\n",
       " 267: 'high',\n",
       " 268: 'video',\n",
       " 269: 's',\n",
       " 270: 'sure',\n",
       " 271: 'lot',\n",
       " 272: 'Your',\n",
       " 273: 'again',\n",
       " 274: 'More',\n",
       " 275: 'number',\n",
       " 276: 'service',\n",
       " 277: 'three',\n",
       " 278: 'including',\n",
       " 279: 'give',\n",
       " 280: 'She',\n",
       " 281: 'during',\n",
       " 282: 'show',\n",
       " 283: 'end',\n",
       " 284: 'family',\n",
       " 285: 'looking',\n",
       " 286: '2013',\n",
       " 287: 'thing',\n",
       " 288: 'name',\n",
       " 289: 'One',\n",
       " 290: 'post',\n",
       " 291: 'might',\n",
       " 292: 'today',\n",
       " 293: '....',\n",
       " 294: 'week',\n",
       " 295: '15',\n",
       " 296: 'old',\n",
       " 297: 'why',\n",
       " 298: 'put',\n",
       " 299: 'Do',\n",
       " 300: 'reblogged',\n",
       " 301: 'read',\n",
       " 302: '11',\n",
       " 303: '9',\n",
       " 304: 'company',\n",
       " 305: 'money',\n",
       " 306: 'top',\n",
       " 307: '30',\n",
       " 308: 'big',\n",
       " 309: 'man',\n",
       " 310: 'book',\n",
       " 311: 'area',\n",
       " 312: 'full',\n",
       " 313: 'page',\n",
       " 314: 'keep',\n",
       " 315: '\\\\',\n",
       " 316: 'away',\n",
       " 317: 'against',\n",
       " 318: '2007',\n",
       " 319: '•',\n",
       " 320: 'small',\n",
       " 321: 'feel',\n",
       " 322: 'support',\n",
       " 323: 'ca',\n",
       " 324: 'real',\n",
       " 325: 'Of',\n",
       " 326: 'times',\n",
       " 327: \"'d\",\n",
       " 328: 'says',\n",
       " 329: 'school',\n",
       " 330: 'play',\n",
       " 331: 'left',\n",
       " 332: 'team',\n",
       " 333: 'water',\n",
       " 334: 'point',\n",
       " 335: 'data',\n",
       " 336: 'music',\n",
       " 337: 'Our',\n",
       " 338: 'ever',\n",
       " 339: 'having',\n",
       " 340: 'News',\n",
       " 341: 'second',\n",
       " 342: 'able',\n",
       " 343: 'order',\n",
       " 344: 'children',\n",
       " 345: 'within',\n",
       " 346: 'City',\n",
       " 347: 'Now',\n",
       " 348: 'though',\n",
       " 349: 'start',\n",
       " 350: 'American',\n",
       " 351: 'months',\n",
       " 352: 'services',\n",
       " 353: 'experience',\n",
       " 354: 'car',\n",
       " 355: 'until',\n",
       " 356: 'University',\n",
       " 357: 'case',\n",
       " 358: 'making',\n",
       " 359: 'x',\n",
       " 360: 'getting',\n",
       " 361: 'March',\n",
       " 362: 'done',\n",
       " 363: 'God',\n",
       " 364: 'state',\n",
       " 365: 'provide',\n",
       " 366: 'course',\n",
       " 367: 'At',\n",
       " 368: 'list',\n",
       " 369: 'June',\n",
       " 370: 'change',\n",
       " 371: 'local',\n",
       " 372: 'enough',\n",
       " 373: 'night',\n",
       " 374: '100',\n",
       " 375: 'hard',\n",
       " 376: 'Free',\n",
       " 377: 'person',\n",
       " 378: 'April',\n",
       " 379: '18',\n",
       " 380: '14',\n",
       " 381: 'let',\n",
       " 382: 'line',\n",
       " 383: 'From',\n",
       " 384: '13',\n",
       " 385: 'website',\n",
       " 386: '16',\n",
       " 387: 'least',\n",
       " 388: 'live',\n",
       " 389: 'power',\n",
       " 390: 'try',\n",
       " 391: '25',\n",
       " 392: 'doing',\n",
       " 393: 'important',\n",
       " 394: 'working',\n",
       " 395: 'less',\n",
       " 396: 'quality',\n",
       " 397: 'group',\n",
       " 398: 'care',\n",
       " 399: 'makes',\n",
       " 400: 'thought',\n",
       " 401: 'side',\n",
       " 402: 'call',\n",
       " 403: 'United',\n",
       " 404: 'per',\n",
       " 405: 'room',\n",
       " 406: 'actually',\n",
       " 407: 'These',\n",
       " 408: 'program',\n",
       " 409: 'public',\n",
       " 410: 'John',\n",
       " 411: 'products',\n",
       " 412: 'January',\n",
       " 413: 'following',\n",
       " 414: 'job',\n",
       " 415: 'yet',\n",
       " 416: 'called',\n",
       " 417: 'buy',\n",
       " 418: '24',\n",
       " 419: 'hours',\n",
       " 420: '!!',\n",
       " 421: 'already',\n",
       " 422: 'Is',\n",
       " 423: 'price',\n",
       " 424: '~',\n",
       " 425: 'July',\n",
       " 426: 'easy',\n",
       " 427: 'design',\n",
       " 428: 'far',\n",
       " 429: 'World',\n",
       " 430: 'please',\n",
       " 431: 'State',\n",
       " 432: 'THE',\n",
       " 433: '@',\n",
       " 434: 'include',\n",
       " 435: 'problem',\n",
       " 436: 'open',\n",
       " 437: 'health',\n",
       " 438: 'food',\n",
       " 439: 'women',\n",
       " 440: 'US',\n",
       " 441: 'based',\n",
       " 442: 'went',\n",
       " 443: 'body',\n",
       " 444: 'County',\n",
       " 445: 'house',\n",
       " 446: 'process',\n",
       " 447: 'needs',\n",
       " 448: 'story',\n",
       " 449: 'York',\n",
       " 450: 'search',\n",
       " 451: 'large',\n",
       " 452: 'February',\n",
       " 453: 'Not',\n",
       " 454: 'together',\n",
       " 455: 'came',\n",
       " 456: 'bit',\n",
       " 457: 'After',\n",
       " 458: 'someone',\n",
       " 459: 'run',\n",
       " 460: '2006',\n",
       " 461: 'Here',\n",
       " 462: 'others',\n",
       " 463: 'October',\n",
       " 464: 'de',\n",
       " 465: 'anything',\n",
       " 466: 'often',\n",
       " 467: 'fact',\n",
       " 468: 'blog',\n",
       " 469: '17',\n",
       " 470: 'means',\n",
       " 471: 'offer',\n",
       " 472: 'country',\n",
       " 473: 'control',\n",
       " 474: 'once',\n",
       " 475: 'November',\n",
       " 476: 'friends',\n",
       " 477: 'view',\n",
       " 478: 'become',\n",
       " 479: 'September',\n",
       " 480: 'possible',\n",
       " 481: 'several',\n",
       " 482: 'Black',\n",
       " 483: 'along',\n",
       " 484: 'bad',\n",
       " 485: 'market',\n",
       " 486: 'city',\n",
       " 487: 'fun',\n",
       " 488: 'believe',\n",
       " 489: 'December',\n",
       " 490: 'nice',\n",
       " 491: 'pretty',\n",
       " 492: 'Please',\n",
       " 493: 'product',\n",
       " 494: 'August',\n",
       " 495: 'seen',\n",
       " 496: 'Day',\n",
       " 497: 'web',\n",
       " 498: 'form',\n",
       " 499: 'took',\n",
       " 500: '£',\n",
       " 501: 'results',\n",
       " 502: 'View',\n",
       " 503: 'Just',\n",
       " 504: 'points',\n",
       " 505: 'students',\n",
       " 506: '21',\n",
       " 507: 'games',\n",
       " 508: 'Center',\n",
       " 509: 'past',\n",
       " 510: 'started',\n",
       " 511: 'School',\n",
       " 512: '19',\n",
       " 513: 'South',\n",
       " 514: 'tell',\n",
       " 515: 'level',\n",
       " 516: '·',\n",
       " 517: 'Business',\n",
       " 518: 'news',\n",
       " 519: '22',\n",
       " 520: 'Comments',\n",
       " 521: 'government',\n",
       " 522: 'kind',\n",
       " 523: 'members',\n",
       " 524: 'comes',\n",
       " 525: 'head',\n",
       " 526: 'light',\n",
       " 527: 'Price',\n",
       " 528: 'check',\n",
       " 529: 'hand',\n",
       " 530: 'minutes',\n",
       " 531: 'North',\n",
       " 532: 'TV',\n",
       " 533: 'U.S.',\n",
       " 534: 'hope',\n",
       " 535: 'National',\n",
       " 536: 'black',\n",
       " 537: 'type',\n",
       " 538: '50',\n",
       " 539: 'above',\n",
       " 540: 'whole',\n",
       " 541: 'probably',\n",
       " 542: 'software',\n",
       " 543: 'season',\n",
       " 544: 'everything',\n",
       " 545: 'below',\n",
       " 546: 'Thanks',\n",
       " 547: 'either',\n",
       " 548: 'Read',\n",
       " 549: 'example',\n",
       " 550: 'four',\n",
       " 551: 'community',\n",
       " 552: 'trying',\n",
       " 553: 'Video',\n",
       " 554: 'special',\n",
       " 555: 'review',\n",
       " 556: 'anyone',\n",
       " 557: 'million',\n",
       " 558: 'idea',\n",
       " 559: 'States',\n",
       " 560: 'access',\n",
       " 561: 'posted',\n",
       " 562: 'added',\n",
       " 563: 'contact',\n",
       " 564: 'add',\n",
       " 565: '!!!',\n",
       " 566: 'men',\n",
       " 567: 'An',\n",
       " 568: 'given',\n",
       " 569: 'However',\n",
       " 570: 'everyone',\n",
       " 571: 'Search',\n",
       " 572: 'value',\n",
       " 573: 'Last',\n",
       " 574: 'phone',\n",
       " 575: 'quite',\n",
       " 576: 'email',\n",
       " 577: 'House',\n",
       " 578: 'nothing',\n",
       " 579: '23',\n",
       " 580: 'known',\n",
       " 581: 'early',\n",
       " 582: 'near',\n",
       " 583: 'download',\n",
       " 584: 'current',\n",
       " 585: 'First',\n",
       " 586: 'pay',\n",
       " 587: 'Best',\n",
       " 588: 'later',\n",
       " 589: 'research',\n",
       " 590: 'mind',\n",
       " 591: 'question',\n",
       " 592: 'personal',\n",
       " 593: 'See',\n",
       " 594: 'project',\n",
       " 595: '2005',\n",
       " 596: '>>',\n",
       " 597: 'young',\n",
       " 598: 'plan',\n",
       " 599: 'told',\n",
       " 600: 'offers',\n",
       " 601: 'almost',\n",
       " 602: 'development',\n",
       " 603: 'movie',\n",
       " 604: 'Internet',\n",
       " 605: 'Some',\n",
       " 606: 'true',\n",
       " 607: 'link',\n",
       " 608: 'month',\n",
       " 609: 'Great',\n",
       " 610: 'About',\n",
       " 611: 'version',\n",
       " 612: 'article',\n",
       " 613: 'event',\n",
       " 614: 'create',\n",
       " 615: 'mean',\n",
       " 616: 'future',\n",
       " 617: 'size',\n",
       " 618: 'West',\n",
       " 619: 'taking',\n",
       " 620: 'words',\n",
       " 621: 'face',\n",
       " 622: 'single',\n",
       " 623: 'features',\n",
       " 624: 'Time',\n",
       " 625: 'close',\n",
       " 626: 'white',\n",
       " 627: 'beautiful',\n",
       " 628: 'looks',\n",
       " 629: 'via',\n",
       " 630: 'history',\n",
       " 631: 'Tags',\n",
       " 632: 'reading',\n",
       " 633: 'series',\n",
       " 634: 'America',\n",
       " 635: 'across',\n",
       " 636: 'file',\n",
       " 637: 'cost',\n",
       " 638: 'seems',\n",
       " 639: 'else',\n",
       " 640: 'Why',\n",
       " 641: 'visit',\n",
       " 642: '_',\n",
       " 643: 'short',\n",
       " 644: 'song',\n",
       " 645: '26',\n",
       " 646: 'front',\n",
       " 647: 'works',\n",
       " 648: '28',\n",
       " 649: 'White',\n",
       " 650: 'Park',\n",
       " 651: 'High',\n",
       " 652: 'wanted',\n",
       " 653: 'class',\n",
       " 654: 'Services',\n",
       " 655: 'law',\n",
       " 656: 'coming',\n",
       " 657: 'whether',\n",
       " 658: 'sex',\n",
       " 659: 'share',\n",
       " 660: 'Online',\n",
       " 661: 'problems',\n",
       " 662: 'perfect',\n",
       " 663: ':)',\n",
       " 664: 'Get',\n",
       " 665: 'weeks',\n",
       " 666: 'His',\n",
       " 667: 'film',\n",
       " 668: 'Service',\n",
       " 669: 'low',\n",
       " 670: 'Windows',\n",
       " 671: 'taken',\n",
       " 672: 'Health',\n",
       " 673: 'Reply',\n",
       " 674: 'reason',\n",
       " 675: '27',\n",
       " 676: 'details',\n",
       " 677: 'soon',\n",
       " 678: 'party',\n",
       " 679: 'complete',\n",
       " 680: 'child',\n",
       " 681: 'International',\n",
       " 682: 'companies',\n",
       " 683: 'questions',\n",
       " 684: 'card',\n",
       " 685: 'range',\n",
       " 686: 'report',\n",
       " 687: 'sale',\n",
       " 688: 'rather',\n",
       " 689: 'Re',\n",
       " 690: 'property',\n",
       " 691: 'comments',\n",
       " 692: 'Post',\n",
       " 693: 'Love',\n",
       " 694: 'hot',\n",
       " 695: 'hotel',\n",
       " 696: 'Top',\n",
       " 697: 'friend',\n",
       " 698: 'issue',\n",
       " 699: 'Well',\n",
       " 700: 'understand',\n",
       " 701: 'due',\n",
       " 702: 'learn',\n",
       " 703: 'heart',\n",
       " 704: 'computer',\n",
       " 705: 'office',\n",
       " 706: 'Music',\n",
       " 707: 'kids',\n",
       " 708: 'Web',\n",
       " 709: 'stop',\n",
       " 710: 'especially',\n",
       " 711: 'English',\n",
       " 712: 'watch',\n",
       " 713: 'Friday',\n",
       " 714: 'provides',\n",
       " 715: 'simple',\n",
       " 716: 'art',\n",
       " 717: 'major',\n",
       " 718: 'issues',\n",
       " 719: 'Also',\n",
       " 720: 'UK',\n",
       " 721: 'management',\n",
       " 722: 'move',\n",
       " 723: 'content',\n",
       " 724: 'training',\n",
       " 725: 'building',\n",
       " 726: 'etc.',\n",
       " 727: 'asked',\n",
       " 728: 'miles',\n",
       " 729: 'credit',\n",
       " 730: 'OF',\n",
       " 731: 'couple',\n",
       " 732: 'industry',\n",
       " 733: 'Date',\n",
       " 734: 'original',\n",
       " 735: 'Page',\n",
       " 736: 'age',\n",
       " 737: 'air',\n",
       " 738: 'AND',\n",
       " 739: 'five',\n",
       " 740: 'date',\n",
       " 741: 'Good',\n",
       " 742: 'includes',\n",
       " 743: 'Other',\n",
       " 744: 'C',\n",
       " 745: 'Street',\n",
       " 746: 'study',\n",
       " 747: 'half',\n",
       " 748: 'social',\n",
       " 749: '29',\n",
       " 750: 'shows',\n",
       " 751: 'space',\n",
       " 752: 'photos',\n",
       " 753: 'present',\n",
       " 754: 'happy',\n",
       " 755: 'matter',\n",
       " 756: 'turn',\n",
       " 757: 'needed',\n",
       " 758: 'David',\n",
       " 759: 'gets',\n",
       " 760: 'name@domain.com',\n",
       " 761: 'image',\n",
       " 762: 'performance',\n",
       " 763: 'media',\n",
       " 764: 'Then',\n",
       " 765: 'human',\n",
       " 766: 'simply',\n",
       " 767: 'stay',\n",
       " 768: 'running',\n",
       " 769: 'deal',\n",
       " 770: 'living',\n",
       " 771: 'girl',\n",
       " 772: 'style',\n",
       " 773: 'London',\n",
       " 774: 'provided',\n",
       " 775: 'ask',\n",
       " 776: 'main',\n",
       " 777: 'enjoy',\n",
       " 778: 'member',\n",
       " 779: 'energy',\n",
       " 780: 'required',\n",
       " 781: 'bring',\n",
       " 782: 'further',\n",
       " 783: 'Most',\n",
       " 784: 'Download',\n",
       " 785: 'technology',\n",
       " 786: 'upon',\n",
       " 787: 'Are',\n",
       " 788: 'playing',\n",
       " 789: 'action',\n",
       " 790: 'Add',\n",
       " 791: 'rate',\n",
       " 792: 'San',\n",
       " 793: 'Google',\n",
       " 794: 'comment',\n",
       " 795: 'saw',\n",
       " 796: 'insurance',\n",
       " 797: '2004',\n",
       " 798: 'sound',\n",
       " 799: 'code',\n",
       " 800: 'takes',\n",
       " 801: 'talk',\n",
       " 802: '{',\n",
       " 803: 'Hotel',\n",
       " 804: 'test',\n",
       " 805: 'While',\n",
       " 806: 'Sunday',\n",
       " 807: 'key',\n",
       " 808: 'stuff',\n",
       " 809: 'cannot',\n",
       " 810: 'woman',\n",
       " 811: 'however',\n",
       " 812: 'related',\n",
       " 813: 'win',\n",
       " 814: 'Saturday',\n",
       " 815: 'Review',\n",
       " 816: 'Click',\n",
       " 817: 'Find',\n",
       " 818: 'California',\n",
       " 819: 'various',\n",
       " 820: 'among',\n",
       " 821: 'usually',\n",
       " 822: 'meet',\n",
       " 823: 'parts',\n",
       " 824: 'guys',\n",
       " 825: 'behind',\n",
       " 826: 'word',\n",
       " 827: 'result',\n",
       " 828: 'amount',\n",
       " 829: 'store',\n",
       " 830: 'yourself',\n",
       " 831: 'wo',\n",
       " 832: 'areas',\n",
       " 833: 'position',\n",
       " 834: 'return',\n",
       " 835: 'field',\n",
       " 836: 'interest',\n",
       " 837: 'Red',\n",
       " 838: 'guy',\n",
       " 839: 'books',\n",
       " 840: 'events',\n",
       " 841: 'myself',\n",
       " 842: 'hit',\n",
       " 843: 'longer',\n",
       " 844: 'Yes',\n",
       " 845: 'leave',\n",
       " 846: 'Big',\n",
       " 847: 'items',\n",
       " 848: 'total',\n",
       " 849: 'remember',\n",
       " 850: 'Mr.',\n",
       " 851: 'percent',\n",
       " 852: 'prices',\n",
       " 853: 'wrong',\n",
       " 854: 'address',\n",
       " 855: 'lost',\n",
       " 856: 'message',\n",
       " 857: 'rest',\n",
       " 858: 'natural',\n",
       " 859: 'picture',\n",
       " 860: 'cause',\n",
       " 861: 'general',\n",
       " 862: '}',\n",
       " 863: 'goes',\n",
       " 864: 'created',\n",
       " 865: 'favorite',\n",
       " 866: 'color',\n",
       " 867: 'drive',\n",
       " 868: 'clear',\n",
       " 869: 'outside',\n",
       " 870: 'click',\n",
       " 871: 'Part',\n",
       " 872: 'paper',\n",
       " 873: 'Information',\n",
       " 874: 'Life',\n",
       " 875: 'shall',\n",
       " 876: 'held',\n",
       " 877: 'inside',\n",
       " 878: 'located',\n",
       " 879: 'Buy',\n",
       " 880: 'private',\n",
       " 881: '40',\n",
       " 882: 'systems',\n",
       " 883: 'currently',\n",
       " 884: 'morning',\n",
       " 885: 'model',\n",
       " 886: 'similar',\n",
       " 887: 'user',\n",
       " 888: 'USA',\n",
       " 889: 'maybe',\n",
       " 890: 'designed',\n",
       " 891: 'professional',\n",
       " 892: 'heard',\n",
       " 893: 'Jan',\n",
       " 894: 'internet',\n",
       " 895: 'writing',\n",
       " 896: 'lead',\n",
       " 897: 'hear',\n",
       " 898: 'security',\n",
       " 899: 'application',\n",
       " 900: 'certain',\n",
       " 901: 'cover',\n",
       " 902: 'rights',\n",
       " 903: 'staff',\n",
       " 904: 'Group',\n",
       " 905: 'Full',\n",
       " 906: 'period',\n",
       " 907: 'account',\n",
       " 908: 'sales',\n",
       " 909: 'Can',\n",
       " 910: 'third',\n",
       " 911: 'College',\n",
       " 912: 'pictures',\n",
       " 913: 'common',\n",
       " 914: 'addition',\n",
       " 915: 'Sale',\n",
       " 916: 'Washington',\n",
       " 917: 'subject',\n",
       " 918: 'record',\n",
       " 919: 'likely',\n",
       " 920: 'St.',\n",
       " 921: 'saying',\n",
       " 922: 'Monday',\n",
       " 923: 'sites',\n",
       " 924: 'hair',\n",
       " 925: 'President',\n",
       " 926: 'written',\n",
       " 927: 'changes',\n",
       " 928: '31',\n",
       " 929: 'James',\n",
       " 930: 'Obama',\n",
       " 931: '2003',\n",
       " 932: 'source',\n",
       " 933: 'received',\n",
       " 934: 'continue',\n",
       " 935: 'ready',\n",
       " 936: 'photo',\n",
       " 937: 'customers',\n",
       " 938: 'sense',\n",
       " 939: 'interesting',\n",
       " 940: 'recent',\n",
       " 941: 'Art',\n",
       " 942: 'forward',\n",
       " 943: 'weight',\n",
       " 944: 'popular',\n",
       " 945: 'reviews',\n",
       " 946: 'wrote',\n",
       " 947: 'ways',\n",
       " 948: 'latest',\n",
       " 949: 'Green',\n",
       " 950: 'oil',\n",
       " 951: 'instead',\n",
       " 952: 'players',\n",
       " 953: 'Inc.',\n",
       " 954: 'East',\n",
       " 955: 'Road',\n",
       " 956: 'cut',\n",
       " 957: 'strong',\n",
       " 958: 'users',\n",
       " 959: 'death',\n",
       " 960: 'General',\n",
       " 961: 'recently',\n",
       " 962: 'save',\n",
       " 963: 'thanks',\n",
       " 964: 'late',\n",
       " 965: 'Dr.',\n",
       " 966: 'network',\n",
       " 967: 'red',\n",
       " 968: 'education',\n",
       " 969: 'financial',\n",
       " 970: 'cool',\n",
       " 971: 'included',\n",
       " 972: 'treatment',\n",
       " 973: 'Thank',\n",
       " 974: 'unique',\n",
       " 975: 'Paul',\n",
       " 976: 'send',\n",
       " 977: 'write',\n",
       " 978: 'jobs',\n",
       " 979: 'baby',\n",
       " 980: 'seem',\n",
       " 981: 'Power',\n",
       " 982: 'player',\n",
       " 983: 'Show',\n",
       " 984: 'section',\n",
       " 985: 'Company',\n",
       " 986: 'Photo',\n",
       " 987: 'Like',\n",
       " 988: 'choose',\n",
       " 989: 'thinking',\n",
       " 990: 'amazing',\n",
       " 991: 'choice',\n",
       " 992: 'according',\n",
       " 993: 'Feb',\n",
       " 994: 'Christmas',\n",
       " 995: 'travel',\n",
       " 996: 'Blue',\n",
       " 997: 'wish',\n",
       " 998: 'Let',\n",
       " 999: 'People',\n",
       " ...}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196016"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(position_to_word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy 는 NLTK 와 같은 토크나이저 라이브러리... 참고 http://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy 가 가진 장점이 있지만...커버링하지 못하는 부분이 여전히 존재하여...커스터마이징이 필요한 부분이 있음.\n",
    "spacy_util.py 주석 참고...\n",
    "\n",
    "    # The following way of definining unicode characters that should be\n",
    "    # tokenized is super ugly and I would hope that it can be improved.\n",
    "    # But it is better than not doing it because spacy's tokenizer won't break\n",
    "    # on these  weird characters when it should.\n",
    "    # To get this list, I scraped the train dataset for all unicode-looking\n",
    "    # things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(nlp)\n",
    "nlp.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\"\n",
    "test_tok_context = nlp(test_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tok_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_tok_context.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.\n",
      "token.idx : 0\n",
      "token.text : Super\n",
      "token.idx : 6\n",
      "token.text : Bowl\n",
      "token.idx : 11\n",
      "token.text : 50\n",
      "token.idx : 14\n",
      "token.text : was\n",
      "token.idx : 18\n",
      "token.text : an\n",
      "token.idx : 21\n",
      "token.text : American\n",
      "token.idx : 30\n",
      "token.text : football\n",
      "token.idx : 39\n",
      "token.text : game\n",
      "token.idx : 44\n",
      "token.text : to\n",
      "token.idx : 47\n",
      "token.text : determine\n",
      "token.idx : 57\n",
      "token.text : the\n",
      "token.idx : 61\n",
      "token.text : champion\n",
      "token.idx : 70\n",
      "token.text : of\n",
      "token.idx : 73\n",
      "token.text : the\n",
      "token.idx : 77\n",
      "token.text : National\n",
      "token.idx : 86\n",
      "token.text : Football\n",
      "token.idx : 95\n",
      "token.text : League\n",
      "token.idx : 102\n",
      "token.text : (\n",
      "token.idx : 103\n",
      "token.text : NFL\n",
      "token.idx : 106\n",
      "token.text : )\n",
      "token.idx : 108\n",
      "token.text : for\n",
      "token.idx : 112\n",
      "token.text : the\n",
      "token.idx : 116\n",
      "token.text : 2015\n",
      "token.idx : 121\n",
      "token.text : season\n",
      "token.idx : 127\n",
      "token.text : .\n",
      "The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.\n",
      "token.idx : 129\n",
      "token.text : The\n",
      "token.idx : 133\n",
      "token.text : American\n",
      "token.idx : 142\n",
      "token.text : Football\n",
      "token.idx : 151\n",
      "token.text : Conference\n",
      "token.idx : 162\n",
      "token.text : (\n",
      "token.idx : 163\n",
      "token.text : AFC\n",
      "token.idx : 166\n",
      "token.text : )\n",
      "token.idx : 168\n",
      "token.text : champion\n",
      "token.idx : 177\n",
      "token.text : Denver\n",
      "token.idx : 184\n",
      "token.text : Broncos\n",
      "token.idx : 192\n",
      "token.text : defeated\n",
      "token.idx : 201\n",
      "token.text : the\n",
      "token.idx : 205\n",
      "token.text : National\n",
      "token.idx : 214\n",
      "token.text : Football\n",
      "token.idx : 223\n",
      "token.text : Conference\n",
      "token.idx : 234\n",
      "token.text : (\n",
      "token.idx : 235\n",
      "token.text : NFC\n",
      "token.idx : 238\n",
      "token.text : )\n",
      "token.idx : 240\n",
      "token.text : champion\n",
      "token.idx : 249\n",
      "token.text : Carolina\n",
      "token.idx : 258\n",
      "token.text : Panthers\n",
      "token.idx : 267\n",
      "token.text : 24\n",
      "token.idx : 269\n",
      "token.text : –\n",
      "token.idx : 270\n",
      "token.text : 10\n",
      "token.idx : 273\n",
      "token.text : to\n",
      "token.idx : 276\n",
      "token.text : earn\n",
      "token.idx : 281\n",
      "token.text : their\n",
      "token.idx : 287\n",
      "token.text : third\n",
      "token.idx : 293\n",
      "token.text : Super\n",
      "token.idx : 299\n",
      "token.text : Bowl\n",
      "token.idx : 304\n",
      "token.text : title\n",
      "token.idx : 309\n",
      "token.text : .\n",
      "The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\n",
      "token.idx : 311\n",
      "token.text : The\n",
      "token.idx : 315\n",
      "token.text : game\n",
      "token.idx : 320\n",
      "token.text : was\n",
      "token.idx : 324\n",
      "token.text : played\n",
      "token.idx : 331\n",
      "token.text : on\n",
      "token.idx : 334\n",
      "token.text : February\n",
      "token.idx : 343\n",
      "token.text : 7\n",
      "token.idx : 344\n",
      "token.text : ,\n",
      "token.idx : 346\n",
      "token.text : 2016\n",
      "token.idx : 350\n",
      "token.text : ,\n",
      "token.idx : 352\n",
      "token.text : at\n",
      "token.idx : 355\n",
      "token.text : Levi\n",
      "token.idx : 359\n",
      "token.text : '\n",
      "token.idx : 360\n",
      "token.text : s\n",
      "token.idx : 362\n",
      "token.text : Stadium\n",
      "token.idx : 370\n",
      "token.text : in\n",
      "token.idx : 373\n",
      "token.text : the\n",
      "token.idx : 377\n",
      "token.text : San\n",
      "token.idx : 381\n",
      "token.text : Francisco\n",
      "token.idx : 391\n",
      "token.text : Bay\n",
      "token.idx : 395\n",
      "token.text : Area\n",
      "token.idx : 400\n",
      "token.text : at\n",
      "token.idx : 403\n",
      "token.text : Santa\n",
      "token.idx : 409\n",
      "token.text : Clara\n",
      "token.idx : 414\n",
      "token.text : ,\n",
      "token.idx : 416\n",
      "token.text : California\n",
      "token.idx : 426\n",
      "token.text : .\n",
      "As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "token.idx : 428\n",
      "token.text : As\n",
      "token.idx : 431\n",
      "token.text : this\n",
      "token.idx : 436\n",
      "token.text : was\n",
      "token.idx : 440\n",
      "token.text : the\n",
      "token.idx : 444\n",
      "token.text : 50th\n",
      "token.idx : 449\n",
      "token.text : Super\n",
      "token.idx : 455\n",
      "token.text : Bowl\n",
      "token.idx : 459\n",
      "token.text : ,\n",
      "token.idx : 461\n",
      "token.text : the\n",
      "token.idx : 465\n",
      "token.text : league\n",
      "token.idx : 472\n",
      "token.text : emphasized\n",
      "token.idx : 483\n",
      "token.text : the\n",
      "token.idx : 487\n",
      "token.text : \"\n",
      "token.idx : 488\n",
      "token.text : golden\n",
      "token.idx : 495\n",
      "token.text : anniversary\n",
      "token.idx : 506\n",
      "token.text : \"\n",
      "token.idx : 508\n",
      "token.text : with\n",
      "token.idx : 513\n",
      "token.text : various\n",
      "token.idx : 521\n",
      "token.text : gold\n",
      "token.idx : 525\n",
      "token.text : -\n",
      "token.idx : 526\n",
      "token.text : themed\n",
      "token.idx : 533\n",
      "token.text : initiatives\n",
      "token.idx : 544\n",
      "token.text : ,\n",
      "token.idx : 546\n",
      "token.text : as\n",
      "token.idx : 549\n",
      "token.text : well\n",
      "token.idx : 554\n",
      "token.text : as\n",
      "token.idx : 557\n",
      "token.text : temporarily\n",
      "token.idx : 569\n",
      "token.text : suspending\n",
      "token.idx : 580\n",
      "token.text : the\n",
      "token.idx : 584\n",
      "token.text : tradition\n",
      "token.idx : 594\n",
      "token.text : of\n",
      "token.idx : 597\n",
      "token.text : naming\n",
      "token.idx : 604\n",
      "token.text : each\n",
      "token.idx : 609\n",
      "token.text : Super\n",
      "token.idx : 615\n",
      "token.text : Bowl\n",
      "token.idx : 620\n",
      "token.text : game\n",
      "token.idx : 625\n",
      "token.text : with\n",
      "token.idx : 630\n",
      "token.text : Roman\n",
      "token.idx : 636\n",
      "token.text : numerals\n",
      "token.idx : 645\n",
      "token.text : (\n",
      "token.idx : 646\n",
      "token.text : under\n",
      "token.idx : 652\n",
      "token.text : which\n",
      "token.idx : 658\n",
      "token.text : the\n",
      "token.idx : 662\n",
      "token.text : game\n",
      "token.idx : 667\n",
      "token.text : would\n",
      "token.idx : 673\n",
      "token.text : have\n",
      "token.idx : 678\n",
      "token.text : been\n",
      "token.idx : 683\n",
      "token.text : known\n",
      "token.idx : 689\n",
      "token.text : as\n",
      "token.idx : 692\n",
      "token.text : \"\n",
      "token.idx : 693\n",
      "token.text : Super\n",
      "token.idx : 699\n",
      "token.text : Bowl\n",
      "token.idx : 704\n",
      "token.text : L\"\n",
      "token.idx : 706\n",
      "token.text : )\n",
      "token.idx : 707\n",
      "token.text : ,\n",
      "token.idx : 709\n",
      "token.text : so\n",
      "token.idx : 712\n",
      "token.text : that\n",
      "token.idx : 717\n",
      "token.text : the\n",
      "token.idx : 721\n",
      "token.text : logo\n",
      "token.idx : 726\n",
      "token.text : could\n",
      "token.idx : 732\n",
      "token.text : prominently\n",
      "token.idx : 744\n",
      "token.text : feature\n",
      "token.idx : 752\n",
      "token.text : the\n",
      "token.idx : 756\n",
      "token.text : Arabic\n",
      "token.idx : 763\n",
      "token.text : numerals\n",
      "token.idx : 772\n",
      "token.text : 50\n",
      "token.idx : 774\n",
      "token.text : .\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_tok_context.sents:\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(\"token.idx : \" + str(token.idx))\n",
    "        print(\"token.text : \" + token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ner_dict(doc):\n",
    "    d = {}\n",
    "    for e in doc.ents:\n",
    "        print(\"e.start_char : \"+ str(e.start_char))\n",
    "        d[e.start_char] = e\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy 에서 해당 entity의 start_char 의 의미 = The character offset for the start of the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e.start_char : 0\n",
      "e.start_char : 11\n",
      "e.start_char : 21\n",
      "e.start_char : 73\n",
      "e.start_char : 103\n",
      "e.start_char : 112\n",
      "e.start_char : 129\n",
      "e.start_char : 163\n",
      "e.start_char : 177\n",
      "e.start_char : 201\n",
      "e.start_char : 249\n",
      "e.start_char : 267\n",
      "e.start_char : 287\n",
      "e.start_char : 293\n",
      "e.start_char : 334\n",
      "e.start_char : 355\n",
      "e.start_char : 373\n",
      "e.start_char : 403\n",
      "e.start_char : 416\n",
      "e.start_char : 440\n",
      "e.start_char : 609\n",
      "e.start_char : 630\n",
      "e.start_char : 693\n",
      "e.start_char : 756\n",
      "e.start_char : 772\n"
     ]
    }
   ],
   "source": [
    "test_ctx_ner_dict = _get_ner_dict(test_tok_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Super Bowl,\n",
       " 11: 50,\n",
       " 21: American,\n",
       " 73: the National Football League,\n",
       " 103: NFL,\n",
       " 112: the 2015 season,\n",
       " 129: The American Football Conference,\n",
       " 163: AFC,\n",
       " 177: Denver Broncos,\n",
       " 201: the National Football Conference,\n",
       " 249: Carolina Panthers,\n",
       " 267: 24–10,\n",
       " 287: third,\n",
       " 293: Super Bowl,\n",
       " 334: February 7, 2016,\n",
       " 355: Levi's Stadium,\n",
       " 373: the San Francisco Bay Area,\n",
       " 403: Santa Clara,\n",
       " 416: California,\n",
       " 440: the 50th Super Bowl,\n",
       " 609: Super Bowl,\n",
       " 630: Roman,\n",
       " 693: Super Bowl L\",\n",
       " 756: Arabic,\n",
       " 772: 50}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ctx_ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev-v1.1.json'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.DEV_SQUAD_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Returns (contexts, word_in_question, questions, word_in_context, spans)\\n    contexts: list of lists of integer word ids\\n    word_in_question: list of lists of booleans indicating whether each\\n        word in the context is present in the question\\n    questions: list of lists of integer word ids\\n    word_in_context: list of lists of booleans indicating whether each\\n        word in the question is present in the context\\n    spans: numpy array of shape (num_samples, 2)\\n    question_ids: a list of ints that indicates which question the\\n        given sample is part of. this has the same length as\\n        |contexts| and |questions|. multiple samples may come from\\n        the same question because there are potentially multiple valid\\n        answers for the same question\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Returns (contexts, word_in_question, questions, word_in_context, spans)\n",
    "    contexts: list of lists of integer word ids\n",
    "    word_in_question: list of lists of booleans indicating whether each\n",
    "        word in the context is present in the question\n",
    "    questions: list of lists of integer word ids\n",
    "    word_in_context: list of lists of booleans indicating whether each\n",
    "        word in the question is present in the context\n",
    "    spans: numpy array of shape (num_samples, 2)\n",
    "    question_ids: a list of ints that indicates which question the\n",
    "        given sample is part of. this has the same length as\n",
    "        |contexts| and |questions|. multiple samples may come from\n",
    "        the same question because there are potentially multiple valid\n",
    "        answers for the same question\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_data_from_tokens_list(tokens_list, tokens_ner_dict):\n",
    "    \"\"\"Input: A spaCy doc.\n",
    "\n",
    "       Ouptut: (vocab_ids_list, vocab_ids_set, pos_list, ner_list)\n",
    "    \"\"\"\n",
    "    vocab_ids_list = []\n",
    "    vocab_ids_set = set()\n",
    "    pos_list = []\n",
    "    ner_list = []\n",
    "    for zz in range(len(tokens_list)):\n",
    "        token = tokens_list[zz]\n",
    "        vocab_id = None\n",
    "        token_pos = None\n",
    "        token_ner = None\n",
    "        if not isinstance(token, spacy.tokens.token.Token) and token == _BOS:\n",
    "            vocab_id = vocab.BOS_ID\n",
    "            token_pos = \"bos\"\n",
    "            token_ner = \"bos\"\n",
    "        elif not isinstance(token, spacy.tokens.token.Token) and token == _EOS:\n",
    "            vocab_id = vocab.EOS_ID\n",
    "            token_pos = \"eos\"\n",
    "            token_ner = \"eos\"\n",
    "        else:\n",
    "            word = token.text\n",
    "            vocab_id = vocab.get_id_for_word(word)\n",
    "            token_pos = token.pos_\n",
    "            token_ner = tokens_ner_dict[token.idx].label_ \\\n",
    "                if token.idx in tokens_ner_dict else \"none\"\n",
    "            vocab_ids_set.add(vocab_id)\n",
    "        vocab_ids_list.append(vocab_id)\n",
    "        pos_list.append(pos_categories.get_id_for_word(token_pos))\n",
    "        ner_list.append(ner_categories.get_id_for_word(token_ner))\n",
    "    return vocab_ids_list, vocab_ids_set, pos_list, ner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_add_samples(value_idx, tok_context=None, tok_question=None, qa=None,\n",
    "    ctx_offset_dict=None, ctx_end_offset_dict=None, list_contexts=None,\n",
    "    list_word_in_question=None, list_questions=None,\n",
    "    list_word_in_context=None, spans=None, num_values=None,\n",
    "    question_ids=None,\n",
    "    context_pos=None,\n",
    "    question_pos=None, context_ner=None, question_ner=None,\n",
    "    is_dev=None, ctx_ner_dict=None, qst_ner_dict=None,\n",
    "    psg_ctx=None):\n",
    "    first_answer = True\n",
    "    for answer in qa[\"answers\"]:\n",
    "        answer_start = answer[\"answer_start\"]\n",
    "        text = answer[\"text\"]\n",
    "        answer_end = answer_start + len(text)\n",
    "        tok_start = None\n",
    "        tok_end = None\n",
    "        exact_match = answer_start in ctx_offset_dict and answer_end in ctx_end_offset_dict\n",
    "        if not exact_match:\n",
    "            # Sometimes, the given answer isn't actually in the context.\n",
    "            # If so, find the smallest surrounding text instead.\n",
    "            for z in range(len(tok_context)):\n",
    "                tok = tok_context[z]\n",
    "                if not isinstance(tok, spacy.tokens.token.Token):\n",
    "                    continue\n",
    "                st = tok.idx\n",
    "                end = st + len(tok.text)\n",
    "                if st <= answer_start and answer_start <= end:\n",
    "                    tok_start = tok\n",
    "                    if z == len(tok_context) - 2:\n",
    "                        tok_end = tok\n",
    "                elif tok_start is not None:\n",
    "                    tok_end = tok\n",
    "                    if end >= answer_end:\n",
    "                        break\n",
    "        tok_start = tok_start if tok_start is not None else ctx_offset_dict[answer_start]\n",
    "        tok_end = tok_end if tok_end is not None else ctx_end_offset_dict[answer_end]\n",
    "        tok_start_idx, tok_end_idx = None, None\n",
    "        for z in range(len(tok_context)):\n",
    "            tok = tok_context[z]\n",
    "            if not isinstance(tok, spacy.tokens.token.Token): # BOS, EOS\n",
    "                continue\n",
    "            if tok == tok_start:\n",
    "                tok_start_idx = z\n",
    "            if tok == tok_end:\n",
    "                tok_end_idx = z\n",
    "            if tok_start_idx is not None and tok_end_idx is not None:\n",
    "                break\n",
    "        assert(tok_start_idx is not None)\n",
    "        assert(tok_end_idx is not None)\n",
    "        # For dev, only keep one exmaple per question, and the set of all\n",
    "        # acceptable answers. This reduces the required memory for storing\n",
    "        # data.\n",
    "        if is_dev and not first_answer:\n",
    "            continue\n",
    "        first_answer = False\n",
    "\n",
    "        spans.append([tok_start_idx, tok_end_idx])\n",
    "        question_ids.append(question_id)\n",
    "\n",
    "        ctx_vocab_ids_list, ctx_vocab_ids_set, \\\n",
    "            ctx_pos_list, ctx_ner_list = \\\n",
    "            _parse_data_from_tokens_list(tok_context, ctx_ner_dict)\n",
    "        list_contexts.append(ctx_vocab_ids_list)\n",
    "        context_pos.append(ctx_pos_list)\n",
    "        context_ner.append(ctx_ner_list)\n",
    "\n",
    "        qst_vocab_ids_list, qst_vocab_ids_set, \\\n",
    "            qst_pos_list, qst_ner_list = \\\n",
    "            _parse_data_from_tokens_list(tok_question, qst_ner_dict)\n",
    "        list_questions.append(qst_vocab_ids_list)\n",
    "        question_pos.append(qst_pos_list)\n",
    "        question_ner.append(qst_ner_list)\n",
    "\n",
    "        word_in_question_list = [1 if word_id in qst_vocab_ids_set else 0 for word_id in ctx_vocab_ids_list]\n",
    "        word_in_context_list = [1 if word_id in ctx_vocab_ids_set else 0 for word_id in qst_vocab_ids_list]\n",
    "        list_word_in_question.append(word_in_question_list)\n",
    "        list_word_in_context.append(word_in_context_list)\n",
    "        print(\"Value\", value_idx, \"of\", num_values, \"percent done\",\n",
    "              100 * float(value_idx) / float(num_values), end=\"\\r\")\n",
    "        value_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_num_data_values(dataset):\n",
    "    numb_values = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                numb_values += 1\n",
    "    return numb_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ner_dict(doc):\n",
    "    d = {}\n",
    "    for e in doc.ents:\n",
    "        d[e.start_char] = e\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = constants.DEV_SQUAD_FILE\n",
    "is_dev = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from file ../downloads/dev-v1.1.json\n",
      "length of data :  48\n",
      "Squad DEV DataSet file() ../downloads/dev-v1.1.json)  num_values : 10570\n",
      "Value 0 of 10570 percent done 0.0 of 10570 percent done 0.0 of 10570 percent done 0.0of 10570 percent done 0.00.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n    return RawTrainingData(\\n        list_contexts = list_contexts,\\n        list_word_in_question = list_word_in_question,\\n        list_questions = list_questions,\\n        list_word_in_context = list_word_in_context,\\n        spans = spans,\\n        question_ids = question_ids,\\n        context_pos = context_pos,\\n        question_pos = question_pos,\\n        context_ner = context_ner,\\n        question_ner = question_ner,\\n        question_ids_to_squad_question_id = question_ids_to_squad_question_id,\\n        question_ids_to_passage_context = question_ids_to_passage_context)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.join(download_dir, data_file)\n",
    "print(\"Reading data from file\", filename)\n",
    "with open(filename) as data_file: \n",
    "    data = json.load(data_file)\n",
    "    dataset = data[\"data\"]\n",
    "    print(\"length of data : \" , str(len(dataset)))\n",
    "    num_values = _get_num_data_values(dataset)\n",
    "    print(\"Squad DEV DataSet file()\" , filename + \") \" , \"num_values :\" , num_values)\n",
    "    spans = []\n",
    "    list_contexts = []\n",
    "    list_word_in_question = []\n",
    "    list_questions = []\n",
    "    list_word_in_context = []\n",
    "    question_ids = []\n",
    "    context_pos = []\n",
    "    question_pos = []\n",
    "    context_ner = []\n",
    "    question_ner = []\n",
    "    question_ids_to_squad_question_id = {}\n",
    "    question_ids_to_passage_context = {}\n",
    "    value_idx = 0\n",
    "    for dataset_id in range(len(dataset)):\n",
    "        if dataset_id > 0 and _DEBUG_USE_ONLY_FIRST_ARTICLE:\n",
    "            break\n",
    "        article = dataset[dataset_id]\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            tok_context = nlp(context)\n",
    "            tok_contexts_with_bos_and_eos = []\n",
    "            ctx_ner_dict = _get_ner_dict(tok_context)\n",
    "            assert tok_context is not None\n",
    "            ctx_offset_dict = {}\n",
    "            ctx_end_offset_dict = {}\n",
    "            word_idx_to_text_position = {}\n",
    "\n",
    "            word_idx = 0\n",
    "            for sentence in tok_context.sents:\n",
    "                tok_contexts_with_bos_and_eos.append(_BOS)\n",
    "                word_idx_to_text_position[word_idx] = \\\n",
    "                    TextPosition(0, 0)\n",
    "                word_idx += 1\n",
    "                for token in sentence:\n",
    "                    tok_contexts_with_bos_and_eos.append(token)\n",
    "                    st = token.idx\n",
    "                    end = token.idx + len(token.text)\n",
    "                    ctx_offset_dict[st] = token\n",
    "                    ctx_end_offset_dict[end] = token\n",
    "                    word_idx_to_text_position[word_idx] = \\\n",
    "                        TextPosition(st, end)\n",
    "                    word_idx += 1\n",
    "                tok_contexts_with_bos_and_eos.append(_EOS)\n",
    "                word_idx_to_text_position[word_idx] = \\\n",
    "                    TextPosition(0, 0)\n",
    "                word_idx += 1\n",
    "\n",
    "#                    word_idx = 0\n",
    "#                    tok_contexts_with_bos_and_eos.append(_BOS)\n",
    "#                    word_idx_to_text_position[word_idx] = \\\n",
    "#                        TextPosition(0, 0)\n",
    "#                    word_idx += 1\n",
    "#                    for token in tok_context:\n",
    "#                        tok_contexts_with_bos_and_eos.append(token)\n",
    "#                        st = token.idx\n",
    "#                        end = token.idx + len(token.text)\n",
    "#                        ctx_offset_dict[st] = token\n",
    "#                        ctx_end_offset_dict[end] = token\n",
    "#                        word_idx_to_text_position[word_idx] = \\\n",
    "#                            TextPosition(st, end)\n",
    "#                        word_idx += 1\n",
    "#                    tok_contexts_with_bos_and_eos.append(_EOS)\n",
    "#                    word_idx_to_text_position[word_idx] = \\\n",
    "#                        TextPosition(0, 0)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_id += 1\n",
    "                acceptable_gnd_truths = []\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    acceptable_gnd_truths.append(answer[\"text\"])\n",
    "                question_ids_to_passage_context[question_id] = \\\n",
    "                    PassageContext(context, word_idx_to_text_position,\n",
    "                        acceptable_gnd_truths)\n",
    "                question = qa[\"question\"]\n",
    "                squad_question_id = qa[\"id\"]\n",
    "                assert squad_question_id is not None\n",
    "                question_ids_to_squad_question_id[question_id] = \\\n",
    "                    squad_question_id\n",
    "                tok_question = nlp(question)\n",
    "                tok_question_with_bos_and_eos = []\n",
    "\n",
    "                for sentence in tok_question.sents:\n",
    "                    tok_question_with_bos_and_eos.append(_BOS)\n",
    "                    for token in sentence:\n",
    "                        tok_question_with_bos_and_eos.append(token)\n",
    "                    tok_question_with_bos_and_eos.append(_EOS)\n",
    "\n",
    "#                        tok_question_with_bos_and_eos.append(_BOS)\n",
    "#                        for token in tok_question:\n",
    "#                            tok_question_with_bos_and_eos.append(token)\n",
    "#                        tok_question_with_bos_and_eos.append(_EOS)\n",
    "\n",
    "                qst_ner_dict = _get_ner_dict(tok_question)\n",
    "                assert tok_question is not None\n",
    "                found_answer_in_context = False\n",
    "                found_answer_in_context = _maybe_add_samples(\n",
    "                    value_idx,\n",
    "                    tok_context=tok_contexts_with_bos_and_eos,\n",
    "                    tok_question=tok_question_with_bos_and_eos, qa=qa,\n",
    "                    ctx_offset_dict=ctx_offset_dict,\n",
    "                    ctx_end_offset_dict=ctx_end_offset_dict,\n",
    "                    list_contexts=list_contexts,\n",
    "                    list_word_in_question=list_word_in_question,\n",
    "                    list_questions=list_questions,\n",
    "                    list_word_in_context=list_word_in_context,\n",
    "                    spans=spans, num_values=num_values,\n",
    "                    question_ids=question_ids,\n",
    "                    context_pos=context_pos, question_pos=question_pos,\n",
    "                    context_ner=context_ner, question_ner=question_ner,\n",
    "                    is_dev=is_dev,\n",
    "                    ctx_ner_dict=ctx_ner_dict,\n",
    "                    qst_ner_dict=qst_ner_dict,\n",
    "                    psg_ctx=question_ids_to_passage_context[question_id])\n",
    "    print(\"\")\n",
    "    spans = np.array(spans[:value_idx], dtype=np.int32)\n",
    "\"\"\" \n",
    "    return RawTrainingData(\n",
    "        list_contexts = list_contexts,\n",
    "        list_word_in_question = list_word_in_question,\n",
    "        list_questions = list_questions,\n",
    "        list_word_in_context = list_word_in_context,\n",
    "        spans = spans,\n",
    "        question_ids = question_ids,\n",
    "        context_pos = context_pos,\n",
    "        question_pos = question_pos,\n",
    "        context_ner = context_ner,\n",
    "        question_ner = question_ner,\n",
    "        question_ids_to_squad_question_id = question_ids_to_squad_question_id,\n",
    "        question_ids_to_passage_context = question_ids_to_passage_context)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting DEV dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-837d58dce194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Getting DEV dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dev_raw_data = self._create_train_data_internal(\n\u001b[0m\u001b[1;32m      3\u001b[0m     constants.DEV_SQUAD_FILE, is_dev=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Getting DEV dataset\")\n",
    "dev_raw_data = self._create_train_data_internal(\n",
    "    constants.DEV_SQUAD_FILE, is_dev=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting TRAIN dataset\")\n",
    "train_raw_data = self._create_train_data_internal(\n",
    "    constants.TRAIN_SQUAD_FILE, is_dev=False)\n",
    "print(\"Num NER categories\", self.ner_categories.get_num_categories())\n",
    "print(\"Num POS categories\", self.pos_categories.get_num_categories())\n",
    "\n",
    "max_context_length = max(\n",
    "        max([len(x) for x in train_raw_data.list_contexts]),\n",
    "        max([len(x) for x in dev_raw_data.list_contexts]))\n",
    "\n",
    "max_question_length = max(\n",
    "        max([len(x) for x in train_raw_data.list_questions]),\n",
    "        max([len(x) for x in dev_raw_data.list_questions]))\n",
    "\n",
    "print(\"Saving TRAIN data\")\n",
    "train_file_saver = DatasetFilesSaver(\n",
    "        train_files_wrapper,\n",
    "        max_context_length,\n",
    "        max_question_length,\n",
    "        self.vocab,\n",
    "        train_raw_data)\n",
    "train_file_saver.save()\n",
    "\n",
    "print(\"Saving DEV data\")\n",
    "dev_file_saver = DatasetFilesSaver(\n",
    "        dev_files_wrapper,\n",
    "        max_context_length,\n",
    "        max_question_length,\n",
    "        self.vocab,\n",
    "        dev_raw_data)\n",
    "dev_file_saver.save()\n",
    "\n",
    "print(\"Finished creating training data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iqab-tf14",
   "language": "python",
   "name": "iqab-tf14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
